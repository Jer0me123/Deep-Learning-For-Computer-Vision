{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn\n",
    "# !pip install tensorflow\n",
    "# !pip install matplotlib\n",
    "# !pip install pandas\n",
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def load_data(path_prefix, dataset_name, splits=['train', 'val', 'test']):\n",
    "    X, y = {}, {}\n",
    "\n",
    "    IMG_SIZE = 224 if 'RAFDB' in dataset_name else 120\n",
    "    splits = ['train', 'test'] if 'RAFDB' in dataset_name else splits\n",
    "\n",
    "    Label_Mapping = {0:'Angry', 1:'Disgust', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise', 6:'Neutral', 7:'Contempt'}\n",
    "    \n",
    "    for split in splits:\n",
    "        PATH = os.path.join(path_prefix, dataset_name, split)\n",
    "        X[split], y[split] = [], []\n",
    "        for classes in Label_Mapping.values():\n",
    "            class_path = os.path.join(PATH, classes)\n",
    "            class_numeric = next((k for k, v in Label_Mapping.items() if v == classes), None)\n",
    "            # print(f\"{classes}_{class_numeric}_{class_path}\")\n",
    "            # print('---------------')\n",
    "\n",
    "            try:\n",
    "                for sample in os.listdir(class_path):\n",
    "                    sample_path = os.path.join(class_path, sample)\n",
    "                    image = cv2.imread(sample_path, cv2.IMREAD_COLOR)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "                    X[split].append(image)\n",
    "                    y[split].append(class_numeric)\n",
    "            except:\n",
    "                print(f\"Dataset doesn't have class {classes}, therefore skipping.\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    for split in splits:\n",
    "        X[split] = np.array(X[split])\n",
    "        y[split] = np.array(y[split])\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = load_data('../../Datasets', dataset_name='JAFFE_Structured', splits=['train', 'validation', 'test'])\n",
    "\n",
    "# with h5py.File('ferp.h5', 'w') as dataset: \n",
    "#     for split in X.keys():\n",
    "#         dataset.create_dataset(f'X_{split}', data=X[split])\n",
    "#         dataset.create_dataset(f'y_{split}', data=y[split])\n",
    "\n",
    "# del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.utils import shuffle\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# data_path = '../../Datasets/JAFFE_Structured'\n",
    "\n",
    "# # Define dataset paths\n",
    "# train_dir = f'{data_path}/train'\n",
    "# valid_dir = f'{data_path}/validation'\n",
    "# test_dir = f'{data_path}/test'\n",
    "\n",
    "# def load_data(directory, img_size=(120, 120)):\n",
    "#     \"\"\"\n",
    "#     Load images and labels from a directory containing a labels.csv file.\n",
    "#     Args:\n",
    "#         directory: Path to dataset split directory\n",
    "#         img_size: Target size for image resizing\n",
    "#     Returns:\n",
    "#         Tuple of (images array, labels array)\n",
    "#     \"\"\"\n",
    "#     # Load CSV with image filenames and labels\n",
    "#     labels_path = os.path.join(directory, 'labels.csv')\n",
    "#     df = pd.read_csv(labels_path)\n",
    "    \n",
    "#     # Adjust these column names if your CSV uses different headers\n",
    "#     filenames = df['filename']\n",
    "#     labels = df['label'].values\n",
    "    \n",
    "#     images = []\n",
    "#     for filename in filenames:\n",
    "#         img_path = os.path.join(directory, filename)\n",
    "        \n",
    "#         # Load and preprocess image\n",
    "#         img = Image.open(img_path).convert('RGB')\n",
    "#         img = img.resize(img_size)\n",
    "#         img_array = np.array(img)\n",
    "        \n",
    "#         # Optional normalization (uncomment if needed)\n",
    "#         # img_array = img_array / 255.0\n",
    "        \n",
    "#         images.append(img_array)\n",
    "    \n",
    "#     return np.array(images), labels\n",
    "\n",
    "# # Load all datasets\n",
    "# X_train, y_train = load_data(train_dir)\n",
    "# X_valid, y_valid = load_data(valid_dir)\n",
    "# X_test, y_test = load_data(test_dir)\n",
    "\n",
    "# # Shuffle training data\n",
    "# X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# # Print shapes\n",
    "# print(\"Shape of train_sample: {}\".format(X_train.shape))\n",
    "# print(\"Shape of train_label: {}\".format(y_train.shape))\n",
    "# print(\"Shape of valid_sample: {}\".format(X_valid.shape))\n",
    "# print(\"Shape of valid_label: {}\".format(y_valid.shape))\n",
    "# print(\"Shape of test_sample: {}\".format(X_test.shape))\n",
    "# print(\"Shape of test_label: {}\".format(y_test.shape))\n",
    "\n",
    "# # Calculate class weights\n",
    "# class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "# class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train = load_data('../../Datasets', dataset_name='JAFFE_Structured', splits=['train'])\n",
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 6\n",
    "IMG_SHAPE = (120, 120, 3)\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "TRAIN_EPOCH = 1 #00\n",
    "TRAIN_LR = 1e-3\n",
    "TRAIN_ES_PATIENCE = 0#5\n",
    "TRAIN_LR_PATIENCE = 3\n",
    "TRAIN_MIN_LR = 1e-6\n",
    "TRAIN_DROPOUT = 0.1\n",
    "\n",
    "FT_EPOCH = 500\n",
    "FT_LR = 1e-5\n",
    "FT_LR_DECAY_STEP = 40.0 #80.0\n",
    "FT_LR_DECAY_RATE = 1\n",
    "FT_ES_PATIENCE = 20\n",
    "FT_DROPOUT = 0.2\n",
    "\n",
    "ES_LR_MIN_DELTA = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset doesn't have class Neutral, therefore skipping.\n",
      "Dataset doesn't have class Contempt, therefore skipping.\n",
      "Dataset doesn't have class Neutral, therefore skipping.\n",
      "Dataset doesn't have class Contempt, therefore skipping.\n",
      "Dataset doesn't have class Neutral, therefore skipping.\n",
      "Dataset doesn't have class Contempt, therefore skipping.\n",
      "Shape of train_sample: (128, 120, 120, 3)\n",
      "Shape of train_label: (128,)\n",
      "Shape of valid_sample: (41, 120, 120, 3)\n",
      "Shape of valid_label: (41,)\n",
      "Shape of test_sample: (44, 120, 120, 3)\n",
      "Shape of test_label: (44,)\n",
      "{0: np.float64(1.1851851851851851), 1: np.float64(0.8888888888888888), 2: np.float64(4.266666666666667), 3: np.float64(0.6274509803921569), 4: np.float64(0.927536231884058), 5: np.float64(0.8888888888888888)}\n"
     ]
    }
   ],
   "source": [
    "X, y = load_data('../../Datasets', dataset_name='JAFFE_Structured', splits=['train', 'validation', 'test'])\n",
    "\n",
    "X_train, y_train = shuffle(X['train'], y['train'])\n",
    "X_valid, y_valid = X['validation'], y['validation']\n",
    "X_test, y_test = X['test'], y['test']\n",
    "\n",
    "# X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "print(\"Shape of train_sample: {}\".format(X_train.shape))\n",
    "print(\"Shape of train_label: {}\".format(y_train.shape))\n",
    "print(\"Shape of valid_sample: {}\".format(X_valid.shape))\n",
    "print(\"Shape of valid_label: {}\".format(y_valid.shape))\n",
    "print(\"Shape of test_sample: {}\".format(X_test.shape))\n",
    "print(\"Shape of test_label: {}\".format(y_test.shape))\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.0256 - loss: 1.7947\n",
      "\n",
      "Finetuning ...\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.2850 - loss: 1.7662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Model Building\n",
    "input_layer = tf.keras.Input(shape=IMG_SHAPE, name='universal_input')\n",
    "sample_resizing = tf.keras.layers.Resizing(224, 224, name=\"resize\")\n",
    "data_augmentation = tf.keras.Sequential([tf.keras.layers.RandomFlip(mode='horizontal'), \n",
    "                                        tf.keras.layers.RandomContrast(factor=0.3)], name=\"augmentation\")\n",
    "preprocess_input = tf.keras.applications.mobilenet.preprocess_input\n",
    "\n",
    "backbone = tf.keras.applications.mobilenet.MobileNet(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "backbone.trainable = False\n",
    "base_model = tf.keras.Model(backbone.input, backbone.layers[-29].output, name='base_model')\n",
    "\n",
    "self_attention = tf.keras.layers.Attention(use_scale=True, name='attention')\n",
    "patch_extraction = tf.keras.Sequential([\n",
    "    tf.keras.layers.SeparableConv2D(256, kernel_size=4, strides=4, padding='same', activation='relu'), \n",
    "    tf.keras.layers.SeparableConv2D(256, kernel_size=2, strides=2, padding='valid', activation='relu'), \n",
    "    tf.keras.layers.Conv2D(256, kernel_size=1, strides=1, padding='valid', activation='relu')\n",
    "], name='patch_extraction')\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D(name='gap')\n",
    "pre_classification = tf.keras.Sequential([tf.keras.layers.Dense(32, activation='relu'), \n",
    "                                          tf.keras.layers.BatchNormalization()], name='pre_classification')\n",
    "prediction_layer = tf.keras.layers.Dense(NUM_CLASSES, activation=\"softmax\", name='classification_head')\n",
    "\n",
    "inputs = input_layer\n",
    "x = sample_resizing(inputs)\n",
    "x = data_augmentation(x)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = patch_extraction(x)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(TRAIN_DROPOUT)(x)\n",
    "x = pre_classification(x)\n",
    "x = tf.keras.layers.Reshape((1, 32))(x) #----\n",
    "x = self_attention([x, x])\n",
    "x = tf.keras.layers.Reshape((32,))(x)  #----\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs, name='train-head')\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=TRAIN_LR, global_clipnorm=3.0), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training Procedure\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=TRAIN_ES_PATIENCE, min_delta=ES_LR_MIN_DELTA, restore_best_weights=True)\n",
    "learning_rate_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=TRAIN_LR_PATIENCE, verbose=0, min_delta=ES_LR_MIN_DELTA, min_lr=TRAIN_MIN_LR)\n",
    "\n",
    "class_weights = {cls: float(weight) for cls, weight in class_weights.items()}\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=TRAIN_EPOCH, batch_size=BATCH_SIZE, validation_data=(X_valid, y_valid), verbose=0, \n",
    "                    class_weight=class_weights, callbacks=[early_stopping_callback, learning_rate_callback])\n",
    "\n",
    "\n",
    "\n",
    "# history = model.fit(X_train, y_train, epochs=TRAIN_EPOCH, batch_size=BATCH_SIZE, validation_data=(X_valid, y_valid), verbose=0, callbacks=[early_stopping_callback, learning_rate_callback])\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Model Finetuning\n",
    "print(\"\\nFinetuning ...\")\n",
    "unfreeze = 59\n",
    "base_model.trainable = True\n",
    "fine_tune_from = len(base_model.layers) - unfreeze\n",
    "for layer in base_model.layers[:fine_tune_from]:\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[fine_tune_from:]:\n",
    "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        layer.trainable = False\n",
    "\n",
    "inputs = input_layer\n",
    "x = sample_resizing(inputs)\n",
    "x = data_augmentation(x)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = patch_extraction(x)\n",
    "x = tf.keras.layers.SpatialDropout2D(FT_DROPOUT)(x)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(FT_DROPOUT)(x)\n",
    "x = pre_classification(x)\n",
    "x = tf.keras.layers.Reshape((1, 32))(x)  #----\n",
    "x = self_attention([x, x])\n",
    "x = tf.keras.layers.Reshape((32,))(x)  #----\n",
    "x = tf.keras.layers.Dropout(FT_DROPOUT)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs, name='finetune-backbone')\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=FT_LR, global_clipnorm=3.0), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Custom function to ensure the scheduler output is a float\n",
    "def lr_schedule(epoch, lr):\n",
    "    return float(scheduler(epoch))\n",
    "\n",
    "# Training Procedure\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=ES_LR_MIN_DELTA, patience=FT_ES_PATIENCE, restore_best_weights=True)\n",
    "scheduler = keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=FT_LR, decay_steps=FT_LR_DECAY_STEP, decay_rate=FT_LR_DECAY_RATE)\n",
    "# scheduler_callback = tf.keras.callbacks.LearningRateScheduler(schedule=scheduler)\n",
    "scheduler_callback = tf.keras.callbacks.LearningRateScheduler(schedule=lr_schedule)\n",
    "\n",
    "history_finetune = model.fit(X_train, y_train, epochs=FT_EPOCH, batch_size=BATCH_SIZE, validation_data=(X_valid, y_valid), verbose=0, \n",
    "                             initial_epoch=history.epoch[-TRAIN_ES_PATIENCE], callbacks=[early_stopping_callback, scheduler_callback, tensorboard_callback])\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MastersRepos\\Deep-Learning-For-Computer-Vision\\Models\\PAtt-Lite\\.venv\\lib\\site-packages\\keras\\src\\ops\\nn.py:908: UserWarning: You are using a softmax over axis -1 of a tensor of shape (8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n",
      "c:\\MastersRepos\\Deep-Learning-For-Computer-Vision\\Models\\PAtt-Lite\\.venv\\lib\\site-packages\\keras\\src\\ops\\nn.py:908: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.1629 - loss: 1.7885\n",
      "\n",
      "Finetuning ...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 83\u001b[0m\n\u001b[0;32m     79\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mschedules\u001b[38;5;241m.\u001b[39mInverseTimeDecay(initial_learning_rate\u001b[38;5;241m=\u001b[39mFT_LR, decay_steps\u001b[38;5;241m=\u001b[39mFT_LR_DECAY_STEP, decay_rate\u001b[38;5;241m=\u001b[39mFT_LR_DECAY_RATE)\n\u001b[0;32m     80\u001b[0m scheduler_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mLearningRateScheduler(schedule\u001b[38;5;241m=\u001b[39mscheduler)\n\u001b[0;32m     82\u001b[0m history_finetune \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39mFT_EPOCH, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, validation_data\u001b[38;5;241m=\u001b[39m(X_valid, y_valid), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \n\u001b[1;32m---> 83\u001b[0m                              initial_epoch\u001b[38;5;241m=\u001b[39m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mTRAIN_ES_PATIENCE\u001b[49m\u001b[43m]\u001b[49m, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping_callback, scheduler_callback, tensorboard_callback])\n\u001b[0;32m     84\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n\u001b[0;32m     85\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# # Model Building\n",
    "# input_layer = tf.keras.Input(shape=IMG_SHAPE, name='universal_input')\n",
    "# # sample_resizing = tf.keras.layers.experimental.preprocessing.Resizing(224, 224, name=\"resize\")\n",
    "\n",
    "# sample_resizing = tf.keras.layers.Resizing(224, 224, name=\"resize\")\n",
    "\n",
    "# data_augmentation = tf.keras.Sequential([tf.keras.layers.RandomFlip(mode='horizontal'), \n",
    "#                                         tf.keras.layers.RandomContrast(factor=0.3)], name=\"augmentation\")\n",
    "# preprocess_input = tf.keras.applications.mobilenet.preprocess_input\n",
    "\n",
    "# backbone = tf.keras.applications.mobilenet.MobileNet(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "# backbone.trainable = False\n",
    "# base_model = tf.keras.Model(backbone.input, backbone.layers[-29].output, name='base_model')\n",
    "\n",
    "# self_attention = tf.keras.layers.Attention(use_scale=True, name='attention')\n",
    "# patch_extraction = tf.keras.Sequential([\n",
    "#     tf.keras.layers.SeparableConv2D(256, kernel_size=4, strides=4, padding='same', activation='relu'), \n",
    "#     tf.keras.layers.SeparableConv2D(256, kernel_size=2, strides=2, padding='valid', activation='relu'), \n",
    "#     tf.keras.layers.Conv2D(256, kernel_size=1, strides=1, padding='valid', activation='relu')\n",
    "# ], name='patch_extraction')\n",
    "# global_average_layer = tf.keras.layers.GlobalAveragePooling2D(name='gap')\n",
    "# pre_classification = tf.keras.Sequential([tf.keras.layers.Dense(32, activation='relu'), \n",
    "#                                           tf.keras.layers.BatchNormalization()], name='pre_classification')\n",
    "# prediction_layer = tf.keras.layers.Dense(NUM_CLASSES, activation=\"softmax\", name='classification_head')\n",
    "\n",
    "inputs = input_layer\n",
    "x = sample_resizing(inputs)\n",
    "x = data_augmentation(x)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = patch_extraction(x)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(TRAIN_DROPOUT)(x)\n",
    "x = pre_classification(x)\n",
    "x = tf.keras.layers.Reshape((1, 32))(x) ##---\n",
    "x = self_attention([x, x])\n",
    "x = tf.keras.layers.Reshape((32,))(x) #---\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs, name='train-head')\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=TRAIN_LR, global_clipnorm=3.0), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Training Procedure\n",
    "# early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=TRAIN_ES_PATIENCE, min_delta=ES_LR_MIN_DELTA, restore_best_weights=True)\n",
    "# learning_rate_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=TRAIN_LR_PATIENCE, verbose=0, min_delta=ES_LR_MIN_DELTA, min_lr=TRAIN_MIN_LR)\n",
    "# history = model.fit(X_train, y_train, epochs=TRAIN_EPOCH, batch_size=BATCH_SIZE, validation_data=(X_valid, y_valid), verbose=0, class_weight=class_weights, callbacks=[early_stopping_callback, learning_rate_callback])\n",
    "# test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "# # Model Finetuning\n",
    "# print(\"\\nFinetuning ...\")\n",
    "# unfreeze = 59\n",
    "# base_model.trainable = True\n",
    "# fine_tune_from = len(base_model.layers) - unfreeze\n",
    "# for layer in base_model.layers[:fine_tune_from]:\n",
    "#     layer.trainable = False\n",
    "# for layer in base_model.layers[fine_tune_from:]:\n",
    "#     if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "#         layer.trainable = False\n",
    "\n",
    "# inputs = input_layer\n",
    "# x = sample_resizing(inputs)\n",
    "# x = data_augmentation(x)\n",
    "# x = preprocess_input(x)\n",
    "# x = base_model(x, training=False)\n",
    "# x = patch_extraction(x)\n",
    "# x = tf.keras.layers.SpatialDropout2D(FT_DROPOUT)(x)\n",
    "# x = global_average_layer(x)\n",
    "# x = tf.keras.layers.Dropout(FT_DROPOUT)(x)\n",
    "# x = pre_classification(x)\n",
    "# x = self_attention([x, x])\n",
    "# x = tf.keras.layers.Dropout(FT_DROPOUT)(x)\n",
    "# outputs = prediction_layer(x)\n",
    "# model = tf.keras.Model(inputs, outputs, name='finetune-backbone')\n",
    "# model.compile(optimizer=keras.optimizers.Adam(learning_rate=FT_LR, global_clipnorm=3.0), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Training Procedure\n",
    "# log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=ES_LR_MIN_DELTA, patience=FT_ES_PATIENCE, restore_best_weights=True)\n",
    "# scheduler = keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=FT_LR, decay_steps=FT_LR_DECAY_STEP, decay_rate=FT_LR_DECAY_RATE)\n",
    "# scheduler_callback = tf.keras.callbacks.LearningRateScheduler(schedule=scheduler)\n",
    "\n",
    "# history_finetune = model.fit(X_train, y_train, epochs=FT_EPOCH, batch_size=BATCH_SIZE, validation_data=(X_valid, y_valid), verbose=0, \n",
    "#                              initial_epoch=history.epoch[-TRAIN_ES_PATIENCE], callbacks=[early_stopping_callback, scheduler_callback, tensorboard_callback])\n",
    "# test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "# model.save('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
