{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "# !pip install tqdm\n",
    "# !pip install scikit-learn\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LINK: https://github.com/SainingZhang/DDAMFN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Feature Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, Sequential, Module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class Flatten(Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def l2_norm(input,axis=1):\n",
    "    norm = torch.norm(input,2,axis,True)\n",
    "    output = torch.div(input, norm)\n",
    "    return output\n",
    "\n",
    "class Conv_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "        self.prelu = PReLU(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.prelu(x)\n",
    "        return x\n",
    "\n",
    "class Linear_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Linear_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class Depth_Wise(Module):\n",
    "     def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1):\n",
    "        super(Depth_Wise, self).__init__()\n",
    "        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.conv_dw = Conv_block(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride)\n",
    "        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.residual = residual\n",
    "     def forward(self, x):\n",
    "        if self.residual:\n",
    "            short_cut = x\n",
    "        x = self.conv(x)\n",
    "        x = self.conv_dw(x)\n",
    "        x = self.project(x)\n",
    "        if self.residual:\n",
    "            output = short_cut + x\n",
    "        else:\n",
    "            output = x\n",
    "        return output\n",
    "  \n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "NON_LINEARITY = {\n",
    "    'ReLU': nn.ReLU(inplace=True),\n",
    "    'Swish': Swish(),\n",
    "}        \n",
    "\n",
    "\n",
    "class h_sigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = nn.ReLU6(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "\n",
    "class h_swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "class swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "    \n",
    "class CoordAtt(nn.Module):\n",
    "    def __init__(self, inp, oup, groups=32):\n",
    "        super(CoordAtt, self).__init__()\n",
    "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
    "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "        mip = max(8, inp // groups)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(mip)\n",
    "        self.conv2 = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "        self.relu = h_swish()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        n,c,h,w = x.size()\n",
    "        x_h = self.pool_h(x)\n",
    "        x_w = self.pool_w(x).permute(0, 1, 3, 2)\n",
    "\n",
    "        y = torch.cat([x_h, x_w], dim=2)\n",
    "        y = self.conv1(y)\n",
    "        y = self.bn1(y)\n",
    "        y = self.relu(y) \n",
    "        x_h, x_w = torch.split(y, [h, w], dim=2)\n",
    "        x_w = x_w.permute(0, 1, 3, 2)\n",
    "\n",
    "        x_h = self.conv2(x_h).sigmoid()\n",
    "        x_w = self.conv3(x_w).sigmoid()\n",
    "        x_h = x_h.expand(-1, -1, h, w)\n",
    "        x_w = x_w.expand(-1, -1, h, w)\n",
    "\n",
    "        y = identity * x_w * x_h\n",
    "\n",
    "        return y\n",
    "\n",
    "        \n",
    "class MDConv(Module):\n",
    "    def __init__(self, channels, kernel_size, split_out_channels, stride):\n",
    "        super(MDConv, self).__init__()\n",
    "        self.num_groups = len(kernel_size)\n",
    "        self.split_channels = split_out_channels\n",
    "        self.mixed_depthwise_conv = nn.ModuleList()\n",
    "        for i in range(self.num_groups):\n",
    "            self.mixed_depthwise_conv.append(Conv2d(\n",
    "                self.split_channels[i],\n",
    "                self.split_channels[i],\n",
    "                kernel_size[i],\n",
    "                stride=stride,\n",
    "                padding=kernel_size[i]//2,\n",
    "                groups=self.split_channels[i],\n",
    "                bias=False\n",
    "            ))\n",
    "        self.bn = BatchNorm2d(channels)\n",
    "        self.prelu = PReLU(channels)            \n",
    "       \n",
    "    def forward(self, x):\n",
    "        if self.num_groups == 1:\n",
    "            return self.mixed_depthwise_conv[0](x)\n",
    "\n",
    "        x_split = torch.split(x, self.split_channels, dim=1)\n",
    "        x = [conv(t) for conv, t in zip(self.mixed_depthwise_conv, x_split)]\n",
    "        x = torch.cat(x, dim=1)\n",
    "\n",
    "        return x        \n",
    "      \n",
    "        \n",
    "class Mix_Depth_Wise(Module):\n",
    "     def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1, kernel_size=[3,5,7], split_out_channels=[64,32,32]):\n",
    "        super(Mix_Depth_Wise, self).__init__()\n",
    "        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.conv_dw = MDConv(channels=groups, kernel_size=kernel_size, split_out_channels=split_out_channels, stride=stride)\n",
    "        self.CA = CoordAtt(groups, groups)\n",
    "        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.residual = residual\n",
    "     def forward(self, x):\n",
    "        if self.residual:\n",
    "            short_cut = x\n",
    "        x = self.conv(x)\n",
    "        x = self.conv_dw(x)\n",
    "        x = self.CA(x)\n",
    "        x = self.project(x)\n",
    "        if self.residual:\n",
    "            output = short_cut + x\n",
    "        else:\n",
    "            output = x\n",
    "        return output\n",
    "          \n",
    "class Residual(Module):\n",
    "    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\n",
    "        super(Residual, self).__init__()\n",
    "        modules = []\n",
    "        for _ in range(num_block):\n",
    "            modules.append(Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups))\n",
    "        self.model = Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "class Mix_Residual(Module):\n",
    "    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1), kernel_size=[3,5], split_out_channels=[64,64]):\n",
    "        super(Mix_Residual, self).__init__()\n",
    "        modules = []\n",
    "        for _ in range(num_block):\n",
    "            modules.append(Mix_Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups, kernel_size=kernel_size, split_out_channels=split_out_channels ))\n",
    "        self.model = Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "\n",
    "class MixedFeatureNet(Module):\n",
    "    def __init__(self, embedding_size=256, out_h=7, out_w=7):\n",
    "        super(MixedFeatureNet, self).__init__()\n",
    "        #112x112\n",
    "        self.conv1 = Conv_block(3, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        #56x56\n",
    "        self.conv2_dw = Conv_block(64, 64, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
    "        self.conv_23 = Mix_Depth_Wise(64, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, kernel_size=[3,5,7], split_out_channels=[64,32,32] )\n",
    "        \n",
    "        #28x28\n",
    "        self.conv_3 = Mix_Residual(64, num_block=9, groups=128, kernel=(3, 3), stride=(1, 1), padding=(1, 1), kernel_size=[3,5], split_out_channels=[96,32])\n",
    "        self.conv_34 = Mix_Depth_Wise(64, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, kernel_size=[3,5,7],split_out_channels=[128,64,64] )\n",
    "        \n",
    "        #14x14\n",
    "        self.conv_4 = Mix_Residual(128, num_block=16, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1), kernel_size=[3,5], split_out_channels=[192,64])\n",
    "        self.conv_45 = Mix_Depth_Wise(128, 256, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=512*2, kernel_size=[3,5,7,9],split_out_channels=[128*2,128*2,128*2,128*2] )\n",
    "        #7x7\n",
    "        self.conv_5 = Mix_Residual(256, num_block=6, groups=512, kernel=(3, 3), stride=(1, 1), padding=(1, 1), kernel_size=[3,5,7], split_out_channels=[86*2,85*2,85*2])                \n",
    "        self.conv_6_sep = Conv_block(256, 512, kernel=(1, 1), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(out_h, out_w), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_flatten = Flatten()\n",
    "        self.linear = Linear(512, embedding_size, bias=False)\n",
    "        self.bn = BatchNorm1d(embedding_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2_dw(out)\n",
    "        out = self.conv_23(out)\n",
    "        out = self.conv_3(out)\n",
    "        out = self.conv_34(out)\n",
    "        out = self.conv_4(out)\n",
    "        out = self.conv_45(out)\n",
    "        out = self.conv_5(out)\n",
    "        out = self.conv_6_sep(out)\n",
    "        out = self.conv_6_dw(out)\n",
    "        out = self.conv_6_flatten(out)\n",
    "        out = self.linear(out)\n",
    "        out = self.bn(out)\n",
    "\n",
    "        return l2_norm(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDAMFN++ Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "# from networks import MixedFeatureNet\n",
    "from torch.nn import Module\n",
    "import os\n",
    "class Linear_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Linear_block, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class Flatten(Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "        \n",
    "class DDAMNet(nn.Module):\n",
    "    def __init__(self, num_class=7,num_head=2, pretrained=True):\n",
    "        super(DDAMNet, self).__init__()\n",
    "\n",
    "        # net = MixedFeatureNet.MixedFeatureNet()\n",
    "        net = MixedFeatureNet()\n",
    "                \n",
    "        if pretrained:\n",
    "            net = torch.load(os.path.join('./pretrained/', \"MFN_msceleb.pth\"), weights_only=False)       \n",
    "      \n",
    "        self.features = nn.Sequential(*list(net.children())[:-4])\n",
    "        self.num_head = num_head\n",
    "        for i in range(int(num_head)):\n",
    "            setattr(self,\"cat_head%d\" %(i), CoordAttHead())                  \n",
    "      \n",
    "        self.Linear = Linear_block(512, 512, groups=512, kernel=(7, 7), stride=(1, 1), padding=(0, 0))\n",
    "        self.flatten = Flatten()      \n",
    "        self.fc = nn.Linear(512, num_class)\n",
    "        self.bn = nn.BatchNorm1d(num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        heads = []\n",
    "       \n",
    "        for i in range(self.num_head):\n",
    "            heads.append(getattr(self,\"cat_head%d\" %i)(x))\n",
    "        head_out =heads\n",
    "        \n",
    "        y = heads[0]\n",
    "        \n",
    "        for i in range(1,self.num_head):\n",
    "            y = torch.max(y,heads[i])                     \n",
    "        \n",
    "        y = x*y\n",
    "        y = self.Linear(y)\n",
    "        y = self.flatten(y) \n",
    "        out = self.fc(y)        \n",
    "        return out, x, head_out\n",
    "        \n",
    "class h_sigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = nn.ReLU6(inplace=inplace)\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "                      \n",
    "class h_swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "class CoordAttHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.CoordAtt = CoordAtt(512,512)\n",
    "    def forward(self, x):\n",
    "        ca = self.CoordAtt(x)\n",
    "        return ca  \n",
    "        \n",
    "class CoordAtt(nn.Module):\n",
    "    def __init__(self, inp, oup, groups=32):\n",
    "        super(CoordAtt, self).__init__()\n",
    "      \n",
    "        self.Linear_h = Linear_block(inp, inp, groups=inp, kernel=(1, 7), stride=(1, 1), padding=(0, 0))        \n",
    "        self.Linear_w = Linear_block(inp, inp, groups=inp, kernel=(7, 1), stride=(1, 1), padding=(0, 0))\n",
    "        \n",
    "        mip = max(8, inp // groups)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(mip)\n",
    "        self.conv2 = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "        self.relu = h_swish()\n",
    "        self.Linear = Linear_block(oup, oup, groups=oup, kernel=(7, 7), stride=(1, 1), padding=(0, 0))\n",
    "        self.flatten = Flatten() \n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        n,c,h,w = x.size()\n",
    "        x_h = self.Linear_h(x)\n",
    "        x_w = self.Linear_w(x)\n",
    "        x_w = x_w.permute(0, 1, 3, 2)\n",
    "\n",
    "        y = torch.cat([x_h, x_w], dim=2)\n",
    "        y = self.conv1(y)\n",
    "        y = self.bn1(y)\n",
    "        y = self.relu(y) \n",
    "        x_h, x_w = torch.split(y, [h, w], dim=2)\n",
    "        x_w = x_w.permute(0, 1, 3, 2)\n",
    "\n",
    "        x_h = self.conv2(x_h).sigmoid()\n",
    "        x_w = self.conv3(x_w).sigmoid()\n",
    "        x_h = x_h.expand(-1, -1, h, w)\n",
    "        x_w = x_w.expand(-1, -1, h, w)\n",
    "        \n",
    "        y = x_w * x_h\n",
    " \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        super().load_state_dict(state_dict)\n",
    "        self.base_optimizer.param_groups = self.param_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDAMFN++ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import torch.nn.functional as F\n",
    "# from networks.DDAM import DDAMNet\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from sam import SAM\n",
    "eps = sys.float_info.epsilon\n",
    "\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--raf_path', type=str, default='./data/fer_112_112_v2.0/rafdb/', help='Raf-DB dataset path.')\n",
    "#     parser.add_argument('--batch_size', type=int, default=128, help='Batch size.')\n",
    "#     parser.add_argument('--lr', type=float, default=5e-4, help='Initial learning rate for sgd.')\n",
    "#     parser.add_argument('--workers', default=16, type=int, help='Number of data loading workers.')\n",
    "#     parser.add_argument('--epochs', type=int, default=60, help='Total training epochs.')\n",
    "#     parser.add_argument('--num_head', type=int, default=2, help='Number of attention head.')\n",
    "#     return parser.parse_args()\n",
    "\n",
    "class AttentionLoss(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(AttentionLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        num_head = len(x)\n",
    "        loss = 0\n",
    "        cnt = 0\n",
    "        if num_head > 1:\n",
    "            for i in range(num_head-1):\n",
    "                for j in range(i+1, num_head):\n",
    "                    mse = F.mse_loss(x[i], x[j])\n",
    "                    cnt = cnt+1\n",
    "                    loss = loss+mse\n",
    "            loss = cnt/(loss + eps)\n",
    "        else:\n",
    "            loss = 0\n",
    "        return loss     \n",
    "                \n",
    "# def plot_confusion_matrix(cm, classes,\n",
    "#                           normalize=False,\n",
    "#                           title='Confusion matrix',\n",
    "#                           cmap=plt.cm.Blues):\n",
    "#     \"\"\"\n",
    "#     This function prints and plots the confusion matrix.\n",
    "#     Normalization can be applied by setting `normalize=True`.\n",
    "#     \"\"\"\n",
    "#     if normalize:\n",
    "#         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     plt.title(title, fontsize=16)\n",
    "#     plt.colorbar()\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, rotation=45)\n",
    "#     plt.yticks(tick_marks, classes)\n",
    "\n",
    "#     fmt = '.2f' if normalize else 'd'\n",
    "    \n",
    "#     thresh = cm.max() / 2.\n",
    "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#         plt.text(j, i, format(cm[i, j]*100, fmt)+'%',\n",
    "#                  horizontalalignment=\"center\",\n",
    "#                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "#     plt.ylabel('Actual', fontsize=18)\n",
    "#     plt.xlabel('Predicted', fontsize=18)\n",
    "#     plt.tight_layout()\n",
    "\n",
    "# class_names = ['Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Angry']  \n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral'] #FER\n",
    "\n",
    "def run_training(class_count=7, num_head=2, raf_path='./data/fer_112_112_v2.0/rafdb/', batch_size=128, lr=5e-4, workers=16, epochs=60):\n",
    "    # args = parse_args()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.enabled = True\n",
    "\n",
    "    # model = DDAMNet(num_class=class_count,num_head=args.num_head)\n",
    "    model = DDAMNet(num_class=class_count,num_head=num_head)\n",
    "    model.to(device)\n",
    "\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([\n",
    "                transforms.RandomRotation(5),\n",
    "                transforms.RandomCrop(112, padding=8)\n",
    "  #          ], p=0.2),\n",
    "            ], p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(scale=(0.02,0.25)),\n",
    "        ])   \n",
    "\n",
    "    # train_dataset = datasets.ImageFolder(f'{args.raf_path}/train', transform = data_transforms)   \n",
    "    train_dataset = datasets.ImageFolder(f'{raf_path}/train', transform = data_transforms)   \n",
    "    \n",
    "    print('Whole train set size:', train_dataset.__len__())\n",
    "\n",
    "    # train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "    #                                            batch_size = args.batch_size,\n",
    "    #                                            num_workers = args.workers,\n",
    "    #                                            shuffle = True,  \n",
    "    #                                            pin_memory = True)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            num_workers = workers,\n",
    "                                            shuffle = True,  \n",
    "                                            pin_memory = True)\n",
    "\n",
    "    data_transforms_val = transforms.Compose([\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])   \n",
    "  \n",
    "    # val_dataset = datasets.ImageFolder(f'{args.raf_path}/val', transform = data_transforms_val)\n",
    "    val_dataset = datasets.ImageFolder(f'{raf_path}/validation', transform = data_transforms_val)        \n",
    "\n",
    "    print('Validation set size:', val_dataset.__len__())\n",
    "    \n",
    "    # val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "    #                                            batch_size = args.batch_size,\n",
    "    #                                            num_workers = args.workers,\n",
    "    #                                            shuffle = False,  \n",
    "    #                                            pin_memory = True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            num_workers = workers,\n",
    "                                            shuffle = False,  \n",
    "                                            pin_memory = True)\n",
    "\n",
    "    criterion_cls = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    criterion_at = AttentionLoss()\n",
    "\n",
    "    params = list(model.parameters()) \n",
    "    # #optimizer = torch.optim.SGD(params,lr=args.lr, weight_decay = 1e-4, momentum=0.9)\n",
    "    # # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    # optimizer = SAM(model.parameters(), torch.optim.Adam, lr=args.lr, rho=0.05, adaptive=False, )\n",
    "    optimizer = SAM(model.parameters(), torch.optim.Adam, lr=lr, rho=0.05, adaptive=False)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "    best_acc = 0\n",
    "    # for epoch in tqdm(range(1, args.epochs + 1)):\n",
    "    # for epoch in tqdm(range(1, epochs + 1)):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        running_loss = 0.0\n",
    "        correct_sum = 0\n",
    "        iter_cnt = 0\n",
    "        model.train()\n",
    "\n",
    "        train_progress = tqdm(total=len(train_dataset), desc=f\"Epoch {epoch} Training\", unit='img', leave=True) #---\n",
    "        # for (imgs, targets) in train_loader:\n",
    "\n",
    "        for imgs, targets in tqdm(train_loader, desc=f\"Epoch {epoch} Training\", leave=True):\n",
    "            if iter_cnt % 100 == 0:\n",
    "                print(iter_cnt)\n",
    "            iter_cnt += 1\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            out,feat,heads = model(imgs)\n",
    "            \n",
    "            loss = criterion_cls(out,targets) + 0.1*criterion_at(heads)  \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "            \n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            out,feat,heads = model(imgs)\n",
    "            \n",
    "            loss = criterion_cls(out,targets) + 0.1*criterion_at(heads) \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.second_step(zero_grad=True)            \n",
    "                                    \n",
    "     \n",
    "            \n",
    "            running_loss += loss\n",
    "            _, predicts = torch.max(out, 1)\n",
    "            correct_num = torch.eq(predicts, targets).sum()\n",
    "            correct_sum += correct_num\n",
    "\n",
    "            train_progress.update(len(imgs))  # Update progress by batch size\n",
    "        train_progress.close()\n",
    "\n",
    "        acc = correct_sum.float() / float(train_dataset.__len__())\n",
    "        running_loss = running_loss/iter_cnt\n",
    "        # tqdm.write('[Epoch %d] Training accuracy: %.4f. Loss: %.3f. LR %.6f' % (epoch, acc, running_loss,optimizer.param_groups[0]['lr']))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "            iter_cnt = 0\n",
    "            bingo_cnt = 0\n",
    "            sample_cnt = 0\n",
    "            \n",
    "            ## for calculating balanced accuracy\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    " \n",
    "            model.eval()\n",
    "            # for (imgs, targets) in val_loader:\n",
    "            for imgs, targets in tqdm(val_loader, desc=f\"Epoch {epoch} Validation\", leave=False):\n",
    "                imgs = imgs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                out,feat,heads = model(imgs)\n",
    "                loss = criterion_cls(out,targets)+ 0.1*criterion_at(heads) \n",
    "\n",
    "                running_loss += loss\n",
    "\n",
    "                _, predicts = torch.max(out, 1)\n",
    "                correct_num  = torch.eq(predicts,targets)\n",
    "                bingo_cnt += correct_num.sum().cpu()\n",
    "                sample_cnt += imgs.size(0)\n",
    "                \n",
    "                y_true.append(targets.cpu().numpy())\n",
    "                y_pred.append(predicts.cpu().numpy())\n",
    "\n",
    "                if iter_cnt == 0:\n",
    "                    all_predicted = predicts\n",
    "                    all_targets = targets\n",
    "                else:\n",
    "                    all_predicted = torch.cat((all_predicted, predicts),0)\n",
    "                    all_targets = torch.cat((all_targets, targets),0)                  \n",
    "                iter_cnt+=1        \n",
    "            running_loss = running_loss/iter_cnt   \n",
    "            scheduler.step()\n",
    "\n",
    "            acc = bingo_cnt.float()/float(sample_cnt)\n",
    "            acc = np.around(acc.numpy(),4)\n",
    "            best_acc = max(acc,best_acc)\n",
    "\n",
    "            y_true = np.concatenate(y_true)\n",
    "            y_pred = np.concatenate(y_pred)\n",
    "            balanced_acc = np.around(balanced_accuracy_score(y_true, y_pred),4)\n",
    "\n",
    "            # tqdm.write(\"[Epoch %d] Validation accuracy:%.4f. bacc:%.4f. Loss:%.3f\" % (epoch, acc, balanced_acc, running_loss))\n",
    "            # tqdm.write(\"best_acc:\" + str(best_acc))\n",
    "\n",
    "            # if acc > 0.91 and acc == best_acc:\n",
    "            if acc == best_acc:\n",
    "                torch.save({'iter': epoch,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                             'optimizer_state_dict': optimizer.state_dict(),},\n",
    "                            os.path.join('TrainedModels', \"rafdb_epoch\"+str(epoch)+\"_acc\"+str(acc)+\"_bacc\"+str(balanced_acc)+\".pth\"))\n",
    "                tqdm.write('Model saved.')\n",
    "                \n",
    "                # Compute confusion matrix\n",
    "                matrix = confusion_matrix(all_targets.data.cpu().numpy(), all_predicted.cpu().numpy())\n",
    "                np.set_printoptions(precision=2)\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                # Plot normalized confusion matrix\n",
    "                # plot_confusion_matrix(matrix, classes=class_names, normalize=True, title= 'RAF-DB Confusion Matrix (acc: %0.2f%%)' %(acc*100))\n",
    "                 \n",
    "                plt.savefig(os.path.join('TrainedModels', \"rafdb_epoch\"+str(epoch)+\"_acc\"+str(acc)+\"_bacc\"+str(balanced_acc)+\".png\"))\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "eps = sys.float_info.epsilon\n",
    "\n",
    "\n",
    "class AttentionLoss(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(AttentionLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        num_head = len(x)\n",
    "        loss = 0\n",
    "        cnt = 0\n",
    "        if num_head > 1:\n",
    "            for i in range(num_head-1):\n",
    "                for j in range(i+1, num_head):\n",
    "                    mse = F.mse_loss(x[i], x[j])\n",
    "                    cnt = cnt+1\n",
    "                    loss = loss+mse\n",
    "            loss = cnt/(loss + eps)\n",
    "        else:\n",
    "            loss = 0\n",
    "        return loss     \n",
    " \n",
    "# class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral'] #FER\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral', 'Contempt'] #AffectNEt\n",
    "\n",
    "def run_training(class_count=7, num_head=2, raf_path='./data/fer_112_112_v2.0/rafdb/', batch_size=128, lr=5e-4, workers=16, epochs=60):\n",
    "    # args = parse_args()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.enabled = True\n",
    "\n",
    "    # model = DDAMNet(num_class=class_count,num_head=args.num_head)\n",
    "    model = DDAMNet(num_class=class_count,num_head=num_head)\n",
    "    model.to(device)\n",
    "\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([\n",
    "                transforms.RandomRotation(5),\n",
    "                transforms.RandomCrop(112, padding=8)\n",
    "            ], p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(scale=(0.02,0.25)),\n",
    "        ])   \n",
    "\n",
    "    train_dataset = datasets.ImageFolder(f'{raf_path}/train', transform = data_transforms)   \n",
    "    \n",
    "    print('Whole train set size:', train_dataset.__len__())\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            num_workers = workers,\n",
    "                                            shuffle = True,  \n",
    "                                            pin_memory = True,\n",
    "                                            drop_last=True)\n",
    "\n",
    "    data_transforms_val = transforms.Compose([\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])   \n",
    "  \n",
    "    val_dataset = datasets.ImageFolder(f'{raf_path}/validation', transform = data_transforms_val)        \n",
    "\n",
    "    print('Validation set size:', val_dataset.__len__())\n",
    "\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            num_workers = workers,\n",
    "                                            shuffle = False,  \n",
    "                                            pin_memory = True,\n",
    "                                            drop_last=True)\n",
    "\n",
    "    # criterion_cls = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # criterion_at = AttentionLoss()\n",
    "\n",
    "    #-----------------------------------------------\n",
    "    # Calculate class weights for the training dataset (balanced weights) - Modification\n",
    "    labels = np.array(train_dataset.targets)\n",
    "    classes = np.unique(labels)\n",
    "    class_weights_np = compute_class_weight(class_weight='balanced', classes=classes, y=labels)\n",
    "    class_weights = torch.tensor(class_weights_np, dtype=torch.float).to(device)\n",
    "\n",
    "    print(\"Class weights: \", class_weights)\n",
    "\n",
    "    criterion_cls = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    criterion_at = AttentionLoss()\n",
    "    #-----------------------------------------------\n",
    "\n",
    "    params = list(model.parameters()) \n",
    "    optimizer = SAM(model.parameters(), torch.optim.Adam, lr=lr, rho=0.05, adaptive=False)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "    best_acc = 0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        running_loss = 0.0\n",
    "        correct_sum = 0\n",
    "        iter_cnt = 0\n",
    "        model.train()\n",
    "\n",
    "        # Initialize progress bar once at start of epoch\n",
    "        train_progress = tqdm(total=len(train_dataset), \n",
    "                            desc=f\"Epoch {epoch}/{epochs} Training\", \n",
    "                            unit='img', \n",
    "                            leave=True,\n",
    "                            postfix={'loss': '?', 'acc': '?'})  # Initial placeholder\n",
    "\n",
    "        for imgs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            out,feat,heads = model(imgs)\n",
    "            loss = criterion_cls(out,targets) + 0.1*criterion_at(heads)  \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "            \n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            out,feat,heads = model(imgs)\n",
    "            loss = criterion_cls(out,targets) + 0.1*criterion_at(heads) \n",
    "            optimizer.zero_grad() #Remove?\n",
    "            loss.backward()\n",
    "            optimizer.second_step(zero_grad=True)            \n",
    "                                    \n",
    "     \n",
    "            \n",
    "            running_loss += loss\n",
    "            _, predicts = torch.max(out, 1)\n",
    "            correct_num = torch.eq(predicts, targets).sum()\n",
    "            correct_sum += correct_num\n",
    "            iter_cnt += 1\n",
    "        \n",
    "            # Update progress bar\n",
    "            train_progress.update(len(imgs))\n",
    "            train_progress.set_postfix({\n",
    "                'loss': f\"{running_loss/iter_cnt:.3f}\",\n",
    "                'acc': f\"{correct_sum/(train_progress.n)*100:.1f}%\",\n",
    "                'lr': f\"{optimizer.param_groups[0]['lr']:.1e}\"\n",
    "            }, refresh=False)\n",
    "\n",
    "        train_progress.close()\n",
    "\n",
    "        acc = correct_sum.float() / float(train_dataset.__len__())\n",
    "        running_loss = running_loss/iter_cnt\n",
    "        tqdm.write('[Epoch %d] Training accuracy: %.4f. Loss: %.3f. LR %.6f' % (epoch, acc, running_loss,optimizer.param_groups[0]['lr']))\n",
    "        \n",
    "        scheduler.step() #----\n",
    "\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "            iter_cnt = 0\n",
    "            bingo_cnt = 0\n",
    "            sample_cnt = 0\n",
    "            \n",
    "            ## for calculating balanced accuracy\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    " \n",
    "            model.eval()\n",
    "\n",
    "            # Initialize validation progress bar\n",
    "            val_progress = tqdm(total=len(val_dataset), \n",
    "                       desc=f\"Epoch {epoch} Validation\",\n",
    "                       unit='img',\n",
    "                       leave=False,\n",
    "                       postfix={'val_loss': '?', 'val_acc': '?'})\n",
    "    \n",
    "            for (imgs, targets) in val_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                out,feat,heads = model(imgs)\n",
    "                loss = criterion_cls(out,targets)+ 0.1*criterion_at(heads) \n",
    "                loss_item = loss.item() #---\n",
    "\n",
    "                running_loss += loss\n",
    "                _, predicts = torch.max(out, 1)\n",
    "\n",
    "                correct_num  = torch.eq(predicts,targets)\n",
    "                current_batch_size = imgs.size(0) #--\n",
    "                \n",
    "                bingo_cnt += correct_num.sum().cpu()\n",
    "                sample_cnt += imgs.size(0)\n",
    "                \n",
    "                y_true.append(targets.cpu().numpy())\n",
    "                y_pred.append(predicts.cpu().numpy())\n",
    "\n",
    "                # Update progress bar\n",
    "                val_progress.update(current_batch_size)\n",
    "                val_progress.set_postfix({\n",
    "                    'val_loss': f\"{running_loss/(iter_cnt+1):.3f}\",\n",
    "                    'val_acc': f\"{(bingo_cnt/sample_cnt)*100:.1f}%\"\n",
    "                }, refresh=False)\n",
    "\n",
    "                if iter_cnt == 0:\n",
    "                    all_predicted = predicts\n",
    "                    all_targets = targets\n",
    "                else:\n",
    "                    all_predicted = torch.cat((all_predicted, predicts),0)\n",
    "                    all_targets = torch.cat((all_targets, targets),0)                  \n",
    "                iter_cnt+=1\n",
    "\n",
    "            val_progress.close()  \n",
    "            running_loss = running_loss/iter_cnt   \n",
    "            # scheduler.step()\n",
    "\n",
    "            acc = bingo_cnt.float()/float(sample_cnt)\n",
    "            acc = np.around(acc.numpy(),4)\n",
    "            best_acc = max(acc,best_acc)\n",
    "\n",
    "            y_true = np.concatenate(y_true)\n",
    "            y_pred = np.concatenate(y_pred)\n",
    "            balanced_acc = np.around(balanced_accuracy_score(y_true, y_pred),4)\n",
    "\n",
    "            tqdm.write(\"[Epoch %d] Validation accuracy:%.4f. bacc:%.4f. Loss:%.3f\" % (epoch, acc, balanced_acc, running_loss))\n",
    "            tqdm.write(\"best_acc:\" + str(best_acc))\n",
    "\n",
    "            # if acc > 0.91 and acc == best_acc:\n",
    "            if acc == best_acc:\n",
    "                best_acc = acc\n",
    "\n",
    "                data_set_name = raf_path.rstrip('/').split('/')[-1]\n",
    "                torch.save({'iter': epoch,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                             'optimizer_state_dict': optimizer.state_dict(),},\n",
    "                            os.path.join('TrainedModels', \"DDAMFN_Dataset\"+str(data_set_name)+\"_Batch\"+str(batch_size)+\"_LR\"+str(lr)+\"_Epochs\"+str(epochs)+\".pth\"))\n",
    "                tqdm.write('Model saved.')\n",
    "                \n",
    "                # Compute confusion matrix\n",
    "                # matrix = confusion_matrix(all_targets.data.cpu().numpy(), all_predicted.cpu().numpy())\n",
    "                # np.set_printoptions(precision=2)\n",
    "                # plt.figure(figsize=(10, 8))\n",
    "                # Plot normalized confusion matrix\n",
    "                # plot_confusion_matrix(matrix, classes=class_names, normalize=True, title= 'RAF-DB Confusion Matrix (acc: %0.2f%%)' %(acc*100))\n",
    "                 \n",
    "                # plt.savefig(os.path.join('TrainedModels', \"DDAMFN_Dataset\"+str(data_set_name)+\"_Batch\"+str(batch_size)+\"_LR\"+str(lr)+\"_Epochs\"+str(epochs)+\".png\"))\n",
    "                # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Whole train set size: 37553\n",
      "Validation set size: 800\n",
      "Class weights:  tensor([0.9388, 1.2518, 1.2343, 0.9388, 0.9388, 0.9388, 0.9388, 0.9388],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 Training: 100%|█████████▉| 37552/37553 [10:34<00:00, 59.19img/s, loss=2.417, acc=33.3%, lr=5.0e-04]\n",
      "c:\\MastersRepos\\Deep-Learning-For-Computer-Vision\\Models\\DDAMFN++\\.venv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Training accuracy: 0.3331. Loss: 2.417. LR 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Validation accuracy:0.5450. bacc:0.5450. Loss:1.495\n",
      "best_acc:0.545\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch.serialization\n",
    "\n",
    "# Add the class to the safe globals\n",
    "torch.serialization.add_safe_globals([MixedFeatureNet])\n",
    "\n",
    "# run_training(class_count=7, num_head=2, raf_path='./Datasets/FER_ResEmoteNet/', batch_size=32, lr=0.0005, workers=16, epochs=3)\n",
    "# run_training(class_count=7, num_head=2, raf_path='./Datasets/FER_ResEmoteNet/', batch_size=32, lr=0.0005, workers=12, epochs=1)\n",
    "run_training(class_count=8, num_head=2, raf_path='../../Datasets/AffectNet_Structured_Mapping', batch_size=16, lr=0.0005, workers=6, epochs=1)\n",
    "\n",
    "# run_training(class_count=6, num_head=2, raf_path='./Datasets/JAFFE_ResEmoteNet/', batch_size=32, lr=0.0005, workers=12, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Happy\n",
      "Class probabilities:\n",
      "Angry: 0.0000\n",
      "Disgust: 0.0000\n",
      "Fear: 0.0000\n",
      "Happy: 1.0000\n",
      "Sad: 0.0000\n",
      "Surprise: 0.0000\n",
      "Neutral: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def load_model(checkpoint_path, num_class=7, num_head=2, device='cuda:0'):\n",
    "    # Initialize model\n",
    "    model = DDAMNet(num_class=num_class, num_head=num_head)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint file {checkpoint_path} not found\")\n",
    "        \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def process_image(image_path, model, device='cuda:0'):\n",
    "    # Define transformations (match validation transforms)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = transform(img).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs, _, _ = model(input_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "        _, predicted_idx = torch.max(outputs, 1)\n",
    "    \n",
    "    return {\n",
    "        'class_index': predicted_idx.item(),\n",
    "        'class_label': class_names[predicted_idx.item()],\n",
    "        'probabilities': probabilities.cpu().numpy()\n",
    "    }\n",
    "\n",
    "\n",
    "# Configuration (modify as needed)\n",
    "checkpoint_path = \"TrainedModels/DDAMFN_DatasetFER_ResEmoteNet_Batch32_LR0.0005_Epochs10.pth\"\n",
    "image_path = \"C:\\MastersRepos\\Deep-Learning-For-Computer-Vision\\Models\\DDAMFN++\\Datasets\\FER_ResEmoteNet\\\\test\\\\test_28713_happy.jpg\"\n",
    "# class_names = ['Happy', 'Surprised', 'Sad', 'Angry', 'Disgusted', 'Fear'] #JAFFE\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral'] #FER\n",
    "\n",
    "# Load model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = load_model(checkpoint_path, num_class=7, device=device)\n",
    "\n",
    "# Process image\n",
    "result = process_image(image_path, model, device)\n",
    "\n",
    "print(f\"Predicted class: {result['class_label']}\")\n",
    "print(\"Class probabilities:\")\n",
    "for cls, prob in zip(class_names, result['probabilities']):\n",
    "    print(f\"{cls}: {prob:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
