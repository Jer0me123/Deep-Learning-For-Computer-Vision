{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "# !pip install tqdm\n",
    "# !pip install scikit-learn\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LINK: https://github.com/SainingZhang/DDAMFN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Feature Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, Sequential, Module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class Flatten(Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def l2_norm(input,axis=1):\n",
    "    norm = torch.norm(input,2,axis,True)\n",
    "    output = torch.div(input, norm)\n",
    "    return output\n",
    "\n",
    "class Conv_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "        self.prelu = PReLU(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.prelu(x)\n",
    "        return x\n",
    "\n",
    "class Linear_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Linear_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class Depth_Wise(Module):\n",
    "     def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1):\n",
    "        super(Depth_Wise, self).__init__()\n",
    "        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.conv_dw = Conv_block(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride)\n",
    "        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.residual = residual\n",
    "     def forward(self, x):\n",
    "        if self.residual:\n",
    "            short_cut = x\n",
    "        x = self.conv(x)\n",
    "        x = self.conv_dw(x)\n",
    "        x = self.project(x)\n",
    "        if self.residual:\n",
    "            output = short_cut + x\n",
    "        else:\n",
    "            output = x\n",
    "        return output\n",
    "  \n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "NON_LINEARITY = {\n",
    "    'ReLU': nn.ReLU(inplace=True),\n",
    "    'Swish': Swish(),\n",
    "}        \n",
    "\n",
    "\n",
    "class h_sigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = nn.ReLU6(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "\n",
    "class h_swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "class swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "    \n",
    "class CoordAtt(nn.Module):\n",
    "    def __init__(self, inp, oup, groups=32):\n",
    "        super(CoordAtt, self).__init__()\n",
    "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
    "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "        mip = max(8, inp // groups)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(mip)\n",
    "        self.conv2 = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "        self.relu = h_swish()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        n,c,h,w = x.size()\n",
    "        x_h = self.pool_h(x)\n",
    "        x_w = self.pool_w(x).permute(0, 1, 3, 2)\n",
    "\n",
    "        y = torch.cat([x_h, x_w], dim=2)\n",
    "        y = self.conv1(y)\n",
    "        y = self.bn1(y)\n",
    "        y = self.relu(y) \n",
    "        x_h, x_w = torch.split(y, [h, w], dim=2)\n",
    "        x_w = x_w.permute(0, 1, 3, 2)\n",
    "\n",
    "        x_h = self.conv2(x_h).sigmoid()\n",
    "        x_w = self.conv3(x_w).sigmoid()\n",
    "        x_h = x_h.expand(-1, -1, h, w)\n",
    "        x_w = x_w.expand(-1, -1, h, w)\n",
    "\n",
    "        y = identity * x_w * x_h\n",
    "\n",
    "        return y\n",
    "\n",
    "        \n",
    "class MDConv(Module):\n",
    "    def __init__(self, channels, kernel_size, split_out_channels, stride):\n",
    "        super(MDConv, self).__init__()\n",
    "        self.num_groups = len(kernel_size)\n",
    "        self.split_channels = split_out_channels\n",
    "        self.mixed_depthwise_conv = nn.ModuleList()\n",
    "        for i in range(self.num_groups):\n",
    "            self.mixed_depthwise_conv.append(Conv2d(\n",
    "                self.split_channels[i],\n",
    "                self.split_channels[i],\n",
    "                kernel_size[i],\n",
    "                stride=stride,\n",
    "                padding=kernel_size[i]//2,\n",
    "                groups=self.split_channels[i],\n",
    "                bias=False\n",
    "            ))\n",
    "        self.bn = BatchNorm2d(channels)\n",
    "        self.prelu = PReLU(channels)            \n",
    "       \n",
    "    def forward(self, x):\n",
    "        if self.num_groups == 1:\n",
    "            return self.mixed_depthwise_conv[0](x)\n",
    "\n",
    "        x_split = torch.split(x, self.split_channels, dim=1)\n",
    "        x = [conv(t) for conv, t in zip(self.mixed_depthwise_conv, x_split)]\n",
    "        x = torch.cat(x, dim=1)\n",
    "\n",
    "        return x        \n",
    "      \n",
    "        \n",
    "class Mix_Depth_Wise(Module):\n",
    "     def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1, kernel_size=[3,5,7], split_out_channels=[64,32,32]):\n",
    "        super(Mix_Depth_Wise, self).__init__()\n",
    "        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.conv_dw = MDConv(channels=groups, kernel_size=kernel_size, split_out_channels=split_out_channels, stride=stride)\n",
    "        self.CA = CoordAtt(groups, groups)\n",
    "        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.residual = residual\n",
    "     def forward(self, x):\n",
    "        if self.residual:\n",
    "            short_cut = x\n",
    "        x = self.conv(x)\n",
    "        x = self.conv_dw(x)\n",
    "        x = self.CA(x)\n",
    "        x = self.project(x)\n",
    "        if self.residual:\n",
    "            output = short_cut + x\n",
    "        else:\n",
    "            output = x\n",
    "        return output\n",
    "          \n",
    "class Residual(Module):\n",
    "    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\n",
    "        super(Residual, self).__init__()\n",
    "        modules = []\n",
    "        for _ in range(num_block):\n",
    "            modules.append(Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups))\n",
    "        self.model = Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "class Mix_Residual(Module):\n",
    "    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1), kernel_size=[3,5], split_out_channels=[64,64]):\n",
    "        super(Mix_Residual, self).__init__()\n",
    "        modules = []\n",
    "        for _ in range(num_block):\n",
    "            modules.append(Mix_Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups, kernel_size=kernel_size, split_out_channels=split_out_channels ))\n",
    "        self.model = Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "\n",
    "class MixedFeatureNet(Module):\n",
    "    def __init__(self, embedding_size=256, out_h=7, out_w=7):\n",
    "        super(MixedFeatureNet, self).__init__()\n",
    "        #112x112\n",
    "        self.conv1 = Conv_block(3, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        #56x56\n",
    "        self.conv2_dw = Conv_block(64, 64, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
    "        self.conv_23 = Mix_Depth_Wise(64, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, kernel_size=[3,5,7], split_out_channels=[64,32,32] )\n",
    "        \n",
    "        #28x28\n",
    "        self.conv_3 = Mix_Residual(64, num_block=9, groups=128, kernel=(3, 3), stride=(1, 1), padding=(1, 1), kernel_size=[3,5], split_out_channels=[96,32])\n",
    "        self.conv_34 = Mix_Depth_Wise(64, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, kernel_size=[3,5,7],split_out_channels=[128,64,64] )\n",
    "        \n",
    "        #14x14\n",
    "        self.conv_4 = Mix_Residual(128, num_block=16, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1), kernel_size=[3,5], split_out_channels=[192,64])\n",
    "        self.conv_45 = Mix_Depth_Wise(128, 256, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=512*2, kernel_size=[3,5,7,9],split_out_channels=[128*2,128*2,128*2,128*2] )\n",
    "        #7x7\n",
    "        self.conv_5 = Mix_Residual(256, num_block=6, groups=512, kernel=(3, 3), stride=(1, 1), padding=(1, 1), kernel_size=[3,5,7], split_out_channels=[86*2,85*2,85*2])                \n",
    "        self.conv_6_sep = Conv_block(256, 512, kernel=(1, 1), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(out_h, out_w), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_flatten = Flatten()\n",
    "        self.linear = Linear(512, embedding_size, bias=False)\n",
    "        self.bn = BatchNorm1d(embedding_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2_dw(out)\n",
    "        out = self.conv_23(out)\n",
    "        out = self.conv_3(out)\n",
    "        out = self.conv_34(out)\n",
    "        out = self.conv_4(out)\n",
    "        out = self.conv_45(out)\n",
    "        out = self.conv_5(out)\n",
    "        out = self.conv_6_sep(out)\n",
    "        out = self.conv_6_dw(out)\n",
    "        out = self.conv_6_flatten(out)\n",
    "        out = self.linear(out)\n",
    "        out = self.bn(out)\n",
    "\n",
    "        return l2_norm(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDAMFN++ Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "# from networks import MixedFeatureNet\n",
    "from torch.nn import Module\n",
    "import os\n",
    "class Linear_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Linear_block, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class Flatten(Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "        \n",
    "class DDAMNet(nn.Module):\n",
    "    def __init__(self, num_class=7,num_head=2, pretrained=True):\n",
    "        super(DDAMNet, self).__init__()\n",
    "\n",
    "        # net = MixedFeatureNet.MixedFeatureNet()\n",
    "        net = MixedFeatureNet()\n",
    "                \n",
    "        if pretrained:\n",
    "            net = torch.load(os.path.join('./pretrained/', \"MFN_msceleb.pth\"), weights_only=False)       \n",
    "      \n",
    "        self.features = nn.Sequential(*list(net.children())[:-4])\n",
    "        self.num_head = num_head\n",
    "        for i in range(int(num_head)):\n",
    "            setattr(self,\"cat_head%d\" %(i), CoordAttHead())                  \n",
    "      \n",
    "        self.Linear = Linear_block(512, 512, groups=512, kernel=(7, 7), stride=(1, 1), padding=(0, 0))\n",
    "        self.flatten = Flatten()      \n",
    "        self.fc = nn.Linear(512, num_class)\n",
    "        self.bn = nn.BatchNorm1d(num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        heads = []\n",
    "       \n",
    "        for i in range(self.num_head):\n",
    "            heads.append(getattr(self,\"cat_head%d\" %i)(x))\n",
    "        head_out =heads\n",
    "        \n",
    "        y = heads[0]\n",
    "        \n",
    "        for i in range(1,self.num_head):\n",
    "            y = torch.max(y,heads[i])                     \n",
    "        \n",
    "        y = x*y\n",
    "        y = self.Linear(y)\n",
    "        y = self.flatten(y) \n",
    "        out = self.fc(y)        \n",
    "        return out, x, head_out\n",
    "        \n",
    "class h_sigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = nn.ReLU6(inplace=inplace)\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "                      \n",
    "class h_swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "class CoordAttHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.CoordAtt = CoordAtt(512,512)\n",
    "    def forward(self, x):\n",
    "        ca = self.CoordAtt(x)\n",
    "        return ca  \n",
    "        \n",
    "class CoordAtt(nn.Module):\n",
    "    def __init__(self, inp, oup, groups=32):\n",
    "        super(CoordAtt, self).__init__()\n",
    "      \n",
    "        self.Linear_h = Linear_block(inp, inp, groups=inp, kernel=(1, 7), stride=(1, 1), padding=(0, 0))        \n",
    "        self.Linear_w = Linear_block(inp, inp, groups=inp, kernel=(7, 1), stride=(1, 1), padding=(0, 0))\n",
    "        \n",
    "        mip = max(8, inp // groups)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(mip)\n",
    "        self.conv2 = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "        self.relu = h_swish()\n",
    "        self.Linear = Linear_block(oup, oup, groups=oup, kernel=(7, 7), stride=(1, 1), padding=(0, 0))\n",
    "        self.flatten = Flatten() \n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        n,c,h,w = x.size()\n",
    "        x_h = self.Linear_h(x)\n",
    "        x_w = self.Linear_w(x)\n",
    "        x_w = x_w.permute(0, 1, 3, 2)\n",
    "\n",
    "        y = torch.cat([x_h, x_w], dim=2)\n",
    "        y = self.conv1(y)\n",
    "        y = self.bn1(y)\n",
    "        y = self.relu(y) \n",
    "        x_h, x_w = torch.split(y, [h, w], dim=2)\n",
    "        x_w = x_w.permute(0, 1, 3, 2)\n",
    "\n",
    "        x_h = self.conv2(x_h).sigmoid()\n",
    "        x_w = self.conv3(x_w).sigmoid()\n",
    "        x_h = x_h.expand(-1, -1, h, w)\n",
    "        x_w = x_w.expand(-1, -1, h, w)\n",
    "        \n",
    "        y = x_w * x_h\n",
    " \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        super().load_state_dict(state_dict)\n",
    "        self.base_optimizer.param_groups = self.param_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDAMFN++ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# from tqdm import tqdm\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.utils.data as data\n",
    "# from torchvision import transforms, datasets\n",
    "\n",
    "# from sklearn.metrics import balanced_accuracy_score\n",
    "# import matplotlib.pyplot as plt\n",
    "# import itertools\n",
    "# import torch.nn.functional as F\n",
    "# # from networks.DDAM import DDAMNet\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# # from sam import SAM\n",
    "# eps = sys.float_info.epsilon\n",
    "\n",
    "# # def parse_args():\n",
    "# #     parser = argparse.ArgumentParser()\n",
    "# #     parser.add_argument('--raf_path', type=str, default='./data/fer_112_112_v2.0/rafdb/', help='Raf-DB dataset path.')\n",
    "# #     parser.add_argument('--batch_size', type=int, default=128, help='Batch size.')\n",
    "# #     parser.add_argument('--lr', type=float, default=5e-4, help='Initial learning rate for sgd.')\n",
    "# #     parser.add_argument('--workers', default=16, type=int, help='Number of data loading workers.')\n",
    "# #     parser.add_argument('--epochs', type=int, default=60, help='Total training epochs.')\n",
    "# #     parser.add_argument('--num_head', type=int, default=2, help='Number of attention head.')\n",
    "# #     return parser.parse_args()\n",
    "\n",
    "# class AttentionLoss(nn.Module):\n",
    "#     def __init__(self, ):\n",
    "#         super(AttentionLoss, self).__init__()\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         num_head = len(x)\n",
    "#         loss = 0\n",
    "#         cnt = 0\n",
    "#         if num_head > 1:\n",
    "#             for i in range(num_head-1):\n",
    "#                 for j in range(i+1, num_head):\n",
    "#                     mse = F.mse_loss(x[i], x[j])\n",
    "#                     cnt = cnt+1\n",
    "#                     loss = loss+mse\n",
    "#             loss = cnt/(loss + eps)\n",
    "#         else:\n",
    "#             loss = 0\n",
    "#         return loss     \n",
    "                \n",
    "# # def plot_confusion_matrix(cm, classes,\n",
    "# #                           normalize=False,\n",
    "# #                           title='Confusion matrix',\n",
    "# #                           cmap=plt.cm.Blues):\n",
    "# #     \"\"\"\n",
    "# #     This function prints and plots the confusion matrix.\n",
    "# #     Normalization can be applied by setting `normalize=True`.\n",
    "# #     \"\"\"\n",
    "# #     if normalize:\n",
    "# #         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "# #         print(\"Normalized confusion matrix\")\n",
    "# #     else:\n",
    "# #         print('Confusion matrix, without normalization')\n",
    "\n",
    "# #     print(cm)\n",
    "\n",
    "# #     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "# #     plt.title(title, fontsize=16)\n",
    "# #     plt.colorbar()\n",
    "# #     tick_marks = np.arange(len(classes))\n",
    "# #     plt.xticks(tick_marks, classes, rotation=45)\n",
    "# #     plt.yticks(tick_marks, classes)\n",
    "\n",
    "# #     fmt = '.2f' if normalize else 'd'\n",
    "    \n",
    "# #     thresh = cm.max() / 2.\n",
    "# #     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "# #         plt.text(j, i, format(cm[i, j]*100, fmt)+'%',\n",
    "# #                  horizontalalignment=\"center\",\n",
    "# #                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "# #     plt.ylabel('Actual', fontsize=18)\n",
    "# #     plt.xlabel('Predicted', fontsize=18)\n",
    "# #     plt.tight_layout()\n",
    "\n",
    "# # class_names = ['Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Angry']  \n",
    "# class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral'] #FER\n",
    "\n",
    "# def run_training(class_count=7, num_head=2, raf_path='./data/fer_112_112_v2.0/rafdb/', batch_size=128, lr=5e-4, workers=16, epochs=60):\n",
    "#     # args = parse_args()\n",
    "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     print(device)\n",
    "\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.backends.cudnn.benchmark = True\n",
    "#         torch.backends.cudnn.deterministic = True\n",
    "#         torch.backends.cudnn.enabled = True\n",
    "\n",
    "#     # model = DDAMNet(num_class=class_count,num_head=args.num_head)\n",
    "#     model = DDAMNet(num_class=class_count,num_head=num_head)\n",
    "#     model.to(device)\n",
    "\n",
    "#     data_transforms = transforms.Compose([\n",
    "#         transforms.Resize((112, 112)),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.RandomApply([\n",
    "#                 transforms.RandomRotation(5),\n",
    "#                 transforms.RandomCrop(112, padding=8)\n",
    "#   #          ], p=0.2),\n",
    "#             ], p=0.5),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                  std=[0.229, 0.224, 0.225]),\n",
    "#         transforms.RandomErasing(scale=(0.02,0.25)),\n",
    "#         ])   \n",
    "\n",
    "#     # train_dataset = datasets.ImageFolder(f'{args.raf_path}/train', transform = data_transforms)   \n",
    "#     train_dataset = datasets.ImageFolder(f'{raf_path}/train', transform = data_transforms)   \n",
    "    \n",
    "#     print('Whole train set size:', train_dataset.__len__())\n",
    "\n",
    "#     # train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "#     #                                            batch_size = args.batch_size,\n",
    "#     #                                            num_workers = args.workers,\n",
    "#     #                                            shuffle = True,  \n",
    "#     #                                            pin_memory = True)\n",
    "\n",
    "#     train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "#                                             batch_size = batch_size,\n",
    "#                                             num_workers = workers,\n",
    "#                                             shuffle = True,  \n",
    "#                                             pin_memory = True)\n",
    "\n",
    "#     data_transforms_val = transforms.Compose([\n",
    "#         transforms.Resize((112, 112)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                  std=[0.229, 0.224, 0.225])])   \n",
    "  \n",
    "#     # val_dataset = datasets.ImageFolder(f'{args.raf_path}/val', transform = data_transforms_val)\n",
    "#     val_dataset = datasets.ImageFolder(f'{raf_path}/validation', transform = data_transforms_val)        \n",
    "\n",
    "#     print('Validation set size:', val_dataset.__len__())\n",
    "    \n",
    "#     # val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "#     #                                            batch_size = args.batch_size,\n",
    "#     #                                            num_workers = args.workers,\n",
    "#     #                                            shuffle = False,  \n",
    "#     #                                            pin_memory = True)\n",
    "\n",
    "#     val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "#                                             batch_size = batch_size,\n",
    "#                                             num_workers = workers,\n",
    "#                                             shuffle = False,  \n",
    "#                                             pin_memory = True)\n",
    "\n",
    "#     criterion_cls = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#     criterion_at = AttentionLoss()\n",
    "\n",
    "#     params = list(model.parameters()) \n",
    "#     # #optimizer = torch.optim.SGD(params,lr=args.lr, weight_decay = 1e-4, momentum=0.9)\n",
    "#     # # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "#     # optimizer = SAM(model.parameters(), torch.optim.Adam, lr=args.lr, rho=0.05, adaptive=False, )\n",
    "#     optimizer = SAM(model.parameters(), torch.optim.Adam, lr=lr, rho=0.05, adaptive=False)\n",
    "\n",
    "#     scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "#     best_acc = 0\n",
    "#     # for epoch in tqdm(range(1, args.epochs + 1)):\n",
    "#     # for epoch in tqdm(range(1, epochs + 1)):\n",
    "#     for epoch in range(1, epochs + 1):\n",
    "#         running_loss = 0.0\n",
    "#         correct_sum = 0\n",
    "#         iter_cnt = 0\n",
    "#         model.train()\n",
    "\n",
    "#         train_progress = tqdm(total=len(train_dataset), desc=f\"Epoch {epoch} Training\", unit='img', leave=True) #---\n",
    "#         # for (imgs, targets) in train_loader:\n",
    "\n",
    "#         for imgs, targets in tqdm(train_loader, desc=f\"Epoch {epoch} Training\", leave=True):\n",
    "#             if iter_cnt % 100 == 0:\n",
    "#                 print(iter_cnt)\n",
    "#             iter_cnt += 1\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             imgs = imgs.to(device)\n",
    "#             targets = targets.to(device)\n",
    "            \n",
    "#             out,feat,heads = model(imgs)\n",
    "            \n",
    "#             loss = criterion_cls(out,targets) + 0.1*criterion_at(heads)  \n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.first_step(zero_grad=True)\n",
    "            \n",
    "#             imgs = imgs.to(device)\n",
    "#             targets = targets.to(device)\n",
    "            \n",
    "#             out,feat,heads = model(imgs)\n",
    "            \n",
    "#             loss = criterion_cls(out,targets) + 0.1*criterion_at(heads) \n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.second_step(zero_grad=True)            \n",
    "                                    \n",
    "     \n",
    "            \n",
    "#             running_loss += loss\n",
    "#             _, predicts = torch.max(out, 1)\n",
    "#             correct_num = torch.eq(predicts, targets).sum()\n",
    "#             correct_sum += correct_num\n",
    "\n",
    "#             train_progress.update(len(imgs))  # Update progress by batch size\n",
    "#         train_progress.close()\n",
    "\n",
    "#         acc = correct_sum.float() / float(train_dataset.__len__())\n",
    "#         running_loss = running_loss/iter_cnt\n",
    "#         # tqdm.write('[Epoch %d] Training accuracy: %.4f. Loss: %.3f. LR %.6f' % (epoch, acc, running_loss,optimizer.param_groups[0]['lr']))\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             running_loss = 0.0\n",
    "#             iter_cnt = 0\n",
    "#             bingo_cnt = 0\n",
    "#             sample_cnt = 0\n",
    "            \n",
    "#             ## for calculating balanced accuracy\n",
    "#             y_true = []\n",
    "#             y_pred = []\n",
    " \n",
    "#             model.eval()\n",
    "#             # for (imgs, targets) in val_loader:\n",
    "#             for imgs, targets in tqdm(val_loader, desc=f\"Epoch {epoch} Validation\", leave=False):\n",
    "#                 imgs = imgs.to(device)\n",
    "#                 targets = targets.to(device)\n",
    "                \n",
    "#                 out,feat,heads = model(imgs)\n",
    "#                 loss = criterion_cls(out,targets)+ 0.1*criterion_at(heads) \n",
    "\n",
    "#                 running_loss += loss\n",
    "\n",
    "#                 _, predicts = torch.max(out, 1)\n",
    "#                 correct_num  = torch.eq(predicts,targets)\n",
    "#                 bingo_cnt += correct_num.sum().cpu()\n",
    "#                 sample_cnt += imgs.size(0)\n",
    "                \n",
    "#                 y_true.append(targets.cpu().numpy())\n",
    "#                 y_pred.append(predicts.cpu().numpy())\n",
    "\n",
    "#                 if iter_cnt == 0:\n",
    "#                     all_predicted = predicts\n",
    "#                     all_targets = targets\n",
    "#                 else:\n",
    "#                     all_predicted = torch.cat((all_predicted, predicts),0)\n",
    "#                     all_targets = torch.cat((all_targets, targets),0)                  \n",
    "#                 iter_cnt+=1        \n",
    "#             running_loss = running_loss/iter_cnt   \n",
    "#             scheduler.step()\n",
    "\n",
    "#             acc = bingo_cnt.float()/float(sample_cnt)\n",
    "#             acc = np.around(acc.numpy(),4)\n",
    "#             best_acc = max(acc,best_acc)\n",
    "\n",
    "#             y_true = np.concatenate(y_true)\n",
    "#             y_pred = np.concatenate(y_pred)\n",
    "#             balanced_acc = np.around(balanced_accuracy_score(y_true, y_pred),4)\n",
    "\n",
    "#             # tqdm.write(\"[Epoch %d] Validation accuracy:%.4f. bacc:%.4f. Loss:%.3f\" % (epoch, acc, balanced_acc, running_loss))\n",
    "#             # tqdm.write(\"best_acc:\" + str(best_acc))\n",
    "\n",
    "#             # if acc > 0.91 and acc == best_acc:\n",
    "#             if acc == best_acc:\n",
    "#                 torch.save({'iter': epoch,\n",
    "#                             'model_state_dict': model.state_dict(),\n",
    "#                              'optimizer_state_dict': optimizer.state_dict(),},\n",
    "#                             os.path.join('TrainedModels', \"rafdb_epoch\"+str(epoch)+\"_acc\"+str(acc)+\"_bacc\"+str(balanced_acc)+\".pth\"))\n",
    "#                 tqdm.write('Model saved.')\n",
    "                \n",
    "#                 # Compute confusion matrix\n",
    "#                 matrix = confusion_matrix(all_targets.data.cpu().numpy(), all_predicted.cpu().numpy())\n",
    "#                 np.set_printoptions(precision=2)\n",
    "#                 plt.figure(figsize=(10, 8))\n",
    "#                 # Plot normalized confusion matrix\n",
    "#                 # plot_confusion_matrix(matrix, classes=class_names, normalize=True, title= 'RAF-DB Confusion Matrix (acc: %0.2f%%)' %(acc*100))\n",
    "                 \n",
    "#                 plt.savefig(os.path.join('TrainedModels', \"rafdb_epoch\"+str(epoch)+\"_acc\"+str(acc)+\"_bacc\"+str(balanced_acc)+\".png\"))\n",
    "#                 plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "eps = sys.float_info.epsilon\n",
    "\n",
    "class AttentionLoss(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(AttentionLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        num_head = len(x)\n",
    "        loss = 0\n",
    "        cnt = 0\n",
    "        if num_head > 1:\n",
    "            for i in range(num_head-1):\n",
    "                for j in range(i+1, num_head):\n",
    "                    mse = F.mse_loss(x[i], x[j])\n",
    "                    cnt = cnt+1\n",
    "                    loss = loss+mse\n",
    "            loss = cnt/(loss + eps)\n",
    "        else:\n",
    "            loss = 0\n",
    "        return loss     \n",
    " \n",
    "# class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral'] #FER\n",
    "# class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral', 'Contempt'] #AffectNEt\n",
    "\n",
    "\n",
    "# (data_directory, dataset_name, device, lr, batch_size, momentum, weight_decay, patience, num_epochs)\n",
    "# def TrainDDAMFN_PP(data_directory, device,   class_count=7, num_head=2, raf_path='./data/fer_112_112_v2.0/rafdb/', batch_size=128, lr=5e-4, workers=16, epochs=60):\n",
    "def TrainDDAMFN_PP(data_directory, dataset_name, device, lr=5e-4, batch_size=128, patience=15, num_epochs=60, num_head=2, workers=16):\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.enabled = True\n",
    "\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([\n",
    "                transforms.RandomRotation(5),\n",
    "                transforms.RandomCrop(112, padding=8)\n",
    "            ], p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(scale=(0.02,0.25)),\n",
    "        ])   \n",
    "\n",
    "    train_dataset = datasets.ImageFolder(f'{data_directory}/train', transform = data_transforms)   \n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            num_workers = workers,\n",
    "                                            shuffle = True,  \n",
    "                                            pin_memory = True,\n",
    "                                            drop_last=True)\n",
    "    \n",
    "    print('Whole train set size:', train_dataset.__len__())\n",
    "\n",
    "    data_transforms_val = transforms.Compose([\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])   \n",
    "  \n",
    "    val_dataset = datasets.ImageFolder(f'{data_directory}/validation', transform = data_transforms_val)        \n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            num_workers = workers,\n",
    "                                            shuffle = False,  \n",
    "                                            pin_memory = True,\n",
    "                                            drop_last=True)\n",
    "    \n",
    "    print('Validation set size:', val_dataset.__len__())\n",
    "\n",
    "    num_classes = len(train_dataset.classes)\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "    # model = DDAMNet(num_class=class_count,num_head=args.num_head)\n",
    "    model = DDAMNet(num_class=num_classes, num_head=num_head)\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    # criterion_cls = torch.nn.CrossEntropyLoss()\n",
    "    # criterion_at = AttentionLoss()\n",
    "\n",
    "    #-----------------------------------------------\n",
    "    # Calculate class weights for the training dataset (balanced weights) - Modification\n",
    "    labels = np.array(train_dataset.targets)\n",
    "    classes = np.unique(labels)\n",
    "    class_weights_np = compute_class_weight(class_weight='balanced', classes=classes, y=labels)\n",
    "    class_weights = torch.tensor(class_weights_np, dtype=torch.float).to(device)\n",
    "\n",
    "    print(\"Class weights: \", class_weights)\n",
    "\n",
    "    criterion_cls = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    criterion_at = AttentionLoss()\n",
    "    #-----------------------------------------------\n",
    "\n",
    "    # params = list(model.parameters()) \n",
    "    optimizer = SAM(model.parameters(), torch.optim.Adam, lr=lr, rho=0.05, adaptive=False)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "    patience_counter = 0\n",
    "    best_val_acc = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        running_loss = 0.0\n",
    "        correct_sum = 0\n",
    "        iter_cnt = 0\n",
    "        model.train()\n",
    "\n",
    "        # Initialize progress bar once at start of epoch\n",
    "        train_progress = tqdm(total=len(train_dataset), \n",
    "                            desc=f\"Epoch {epoch}/{num_epochs} Training\", \n",
    "                            unit='img', \n",
    "                            leave=True,\n",
    "                            postfix={'loss': '?', 'acc': '?'})  # Initial placeholder\n",
    "\n",
    "        for imgs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            out,feat,heads = model(imgs)\n",
    "            loss = criterion_cls(out,targets) + 0.1*criterion_at(heads)  \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "            \n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            out,feat,heads = model(imgs)\n",
    "            loss = criterion_cls(out,targets) + 0.1*criterion_at(heads) \n",
    "            # optimizer.zero_grad() #Remove?\n",
    "            loss.backward()\n",
    "            optimizer.second_step(zero_grad=True)            \n",
    "                                    \n",
    "            running_loss += loss\n",
    "            _, predicts = torch.max(out, 1)\n",
    "            correct_num = torch.eq(predicts, targets).sum()\n",
    "            correct_sum += correct_num\n",
    "            iter_cnt += 1\n",
    "        \n",
    "            # Update progress bar\n",
    "            train_progress.update(len(imgs))\n",
    "            train_progress.set_postfix({\n",
    "                'loss': f\"{running_loss/iter_cnt:.3f}\",\n",
    "                'acc': f\"{correct_sum/(train_progress.n)*100:.1f}%\",\n",
    "                'lr': f\"{optimizer.param_groups[0]['lr']:.1e}\"\n",
    "            }, refresh=False)\n",
    "\n",
    "        train_progress.close()\n",
    "\n",
    "        acc = correct_sum.float() / float(train_dataset.__len__())\n",
    "        running_loss = running_loss/iter_cnt\n",
    "        tqdm.write('[Epoch %d] Training accuracy: %.4f. Loss: %.3f. LR %.6f' % (epoch, acc, running_loss,optimizer.param_groups[0]['lr']))\n",
    "        \n",
    "        # scheduler.step() #----\n",
    "\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "            iter_cnt = 0\n",
    "            bingo_cnt = 0\n",
    "            sample_cnt = 0\n",
    "            \n",
    "            # for calculating balanced accuracy\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    " \n",
    "            model.eval()\n",
    "\n",
    "            # Initialize validation progress bar\n",
    "            val_progress = tqdm(total=len(val_dataset), \n",
    "                       desc=f\"Epoch {epoch} Validation\",\n",
    "                       unit='img',\n",
    "                       leave=False,\n",
    "                       postfix={'val_loss': '?', 'val_acc': '?'})\n",
    "    \n",
    "            for (imgs, targets) in val_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                out,feat,heads = model(imgs)\n",
    "                loss = criterion_cls(out,targets)+ 0.1*criterion_at(heads) \n",
    "                loss_item = loss.item() #---\n",
    "\n",
    "                running_loss += loss\n",
    "                _, predicts = torch.max(out, 1)\n",
    "\n",
    "                correct_num  = torch.eq(predicts,targets)\n",
    "                current_batch_size = imgs.size(0) #--\n",
    "                \n",
    "                bingo_cnt += correct_num.sum().cpu()\n",
    "                sample_cnt += imgs.size(0)\n",
    "                \n",
    "                y_true.append(targets.cpu().numpy())\n",
    "                y_pred.append(predicts.cpu().numpy())\n",
    "\n",
    "                # Update progress bar\n",
    "                val_progress.update(current_batch_size)\n",
    "                val_progress.set_postfix({\n",
    "                    'val_loss': f\"{running_loss/(iter_cnt+1):.3f}\",\n",
    "                    'val_acc': f\"{(bingo_cnt/sample_cnt)*100:.1f}%\"\n",
    "                }, refresh=False)\n",
    "\n",
    "                if iter_cnt == 0:\n",
    "                    all_predicted = predicts\n",
    "                    all_targets = targets\n",
    "                else:\n",
    "                    all_predicted = torch.cat((all_predicted, predicts),0)\n",
    "                    all_targets = torch.cat((all_targets, targets),0)                  \n",
    "                iter_cnt+=1\n",
    "\n",
    "            val_progress.close()  \n",
    "            running_loss = running_loss/iter_cnt   \n",
    "            scheduler.step()\n",
    "\n",
    "            acc = bingo_cnt.float()/float(sample_cnt)\n",
    "            acc = np.around(acc.numpy(),4)\n",
    "\n",
    "            y_true = np.concatenate(y_true)\n",
    "            y_pred = np.concatenate(y_pred)\n",
    "            balanced_acc = np.around(balanced_accuracy_score(y_true, y_pred),4)\n",
    "\n",
    "            tqdm.write(\"[Epoch %d] Validation accuracy:%.4f. bacc:%.4f. Loss:%.3f\" % (epoch, acc, balanced_acc, running_loss))\n",
    "\n",
    "\n",
    "            if acc > best_val_acc:\n",
    "                best_val_acc = acc\n",
    "                patience_counter = 0\n",
    "                os.makedirs(\"Models\", exist_ok=True)\n",
    "                torch.save({'iter': epoch,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'workers': workers,\n",
    "                            'num_head': num_head,},\n",
    "                            os.path.join('Models', f\"DDAMFNPP_{dataset_name}_best_model_LR_{lr}_BS_{batch_size}_P_{patience}_E_{num_epochs}_H_{num_head}_W_{workers}.pth\"))\n",
    "                tqdm.write('Model saved.')\n",
    "            else:    \n",
    "                patience_counter += 1\n",
    "                print(f\"No improvement in validation accuracy for {patience_counter} epochs.\")\n",
    "\n",
    "            if patience_counter > patience:\n",
    "                print(\"Stopping early due to lack of improvement in validation accuracy.\")\n",
    "                break\n",
    "\n",
    "    tqdm.write(\"best_acc:\" + str(best_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "dataset_name, data_directory = 'AffectNet', '../../Datasets/AffectNet_Structured'\n",
    "# dataset_name, data_directory = 'CK+', '../../Datasets/CK+_Structured'\n",
    "# dataset_name, data_directory = 'FER', '../../Datasets/FER_Structured'\n",
    "# dataset_name, data_directory = 'JAFFE', '../../Datasets/JAFFE_Structured'\n",
    "# dataset_name, data_directory = 'RAF-DB', '../../Datasets/RAF-DB_Structured'\n",
    "\n",
    "lr = 0.001\n",
    "batch_size = 16\n",
    "patience = 15\n",
    "num_epochs = 1 #80\n",
    "num_head = 2    \n",
    "workers = 16\n",
    "\n",
    "TrainDDAMFN_PP(data_directory, dataset_name, device, lr, batch_size, patience, num_epochs, num_head, workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving Metrics on Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# def TestDDAMFN(data_directory, model_path, num_head=2, batch_size=15):\n",
    "def TestDDAMFN(data_directory, model_path, batch_size=15):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    # Transform the dataset\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225]),\n",
    "    ])   \n",
    "\n",
    "    state_dict = torch.load(model_path, weights_only=True)\n",
    "\n",
    "    final_fc_layer = [k for k in state_dict['model_state_dict'].keys() if 'fc' in k and 'weight' in k][-1]\n",
    "    num_classes = state_dict['model_state_dict'][final_fc_layer].shape[0]\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "    num_head = state_dict['num_head']\n",
    "    print(f\"Number of attention heads: {num_head}\")\n",
    "\n",
    "    # Load your best model (update the filename based on your saved best model)\n",
    "    model = DDAMNet(num_class=num_classes, num_head=num_head).to(device)\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    workers = state_dict['workers']\n",
    "\n",
    "    test_dataset = datasets.ImageFolder(f'{data_directory}/test', transform = transform)   \n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, num_workers = workers, shuffle = False, pin_memory = True)\n",
    "\n",
    "    # Iterate through test_loader to collect predictions and ground truth labels\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs, feat, heads = model(inputs)\n",
    "\n",
    "            # Get the predicted class (assumes outputs are logits)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.append(predicted.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    # Concatenate lists to form single arrays\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision_weighted = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    precision_macro = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    precision_micro = precision_score(all_labels, all_preds, average='micro', zero_division=0)\n",
    "\n",
    "    recall_weighted = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall_macro = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall_micro = recall_score(all_labels, all_preds, average='micro', zero_division=0)\n",
    "\n",
    "    f1_weighted = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n",
    "\n",
    "    report = classification_report(all_labels, all_preds, zero_division=0)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Print the metrics\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    print(\"Precision (Weighted):\", precision_weighted)\n",
    "    print(\"Precision (Macro):\", precision_macro)\n",
    "    print(\"Precision (Micro):\", precision_micro)\n",
    "    print(\"Recall (Weighted):\", recall_weighted)\n",
    "    print(\"Recall (Macro):\", recall_macro)\n",
    "    print(\"Recall (Micro):\", recall_micro)\n",
    "    print(\"F1 Score (Weighted):\", f1_weighted)\n",
    "    print(\"F1 Score (Macro):\", f1_macro)\n",
    "    print(\"F1 Score (Micro):\", f1_micro)\n",
    "    print(\"\\nClassification Report:\\n\", report)\n",
    "    print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "\n",
    "    # (Optional) Visualize the confusion matrix using seaborn heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Number of classes: 6\n",
      "Number of attention heads: 2\n",
      "Test Accuracy: 0.1951219512195122\n",
      "Precision (Weighted): 0.07560975609756097\n",
      "Precision (Macro): 0.061111111111111116\n",
      "Precision (Micro): 0.1951219512195122\n",
      "Recall (Weighted): 0.1951219512195122\n",
      "Recall (Macro): 0.16435185185185186\n",
      "Recall (Micro): 0.1951219512195122\n",
      "F1 Score (Weighted): 0.09279636982416337\n",
      "F1 Score (Macro): 0.0764857881136951\n",
      "F1 Score (Micro): 0.1951219512195122\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         6\n",
      "           1       0.00      0.00      0.00         8\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         8\n",
      "           4       0.20      0.88      0.33         8\n",
      "           5       0.17      0.11      0.13         9\n",
      "\n",
      "    accuracy                           0.20        41\n",
      "   macro avg       0.06      0.16      0.08        41\n",
      "weighted avg       0.08      0.20      0.09        41\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[0 0 0 0 5 1]\n",
      " [0 0 0 0 6 2]\n",
      " [0 0 0 0 2 0]\n",
      " [0 0 0 0 7 1]\n",
      " [0 0 0 0 7 1]\n",
      " [0 0 0 0 8 1]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset_name, data_directory = 'AffectNet', '../../Datasets/AffectNet_Structured'\n",
    "# dataset_name, data_directory = 'CK+', '../../Datasets/CK+_Structured'\n",
    "# dataset_name, data_directory = 'FER', '../../Datasets/FER_Structured'\n",
    "dataset_name, data_directory = 'JAFFE', '../../Datasets/JAFFE_Structured'\n",
    "# dataset_name, data_directory = 'RAF-DB', '../../Datasets/RAF-DB_Structured'\n",
    "\n",
    "lr = 0.001\n",
    "batch_size = 16\n",
    "patience = 15\n",
    "num_epochs = 1 #80\n",
    "num_head = 2\n",
    "workers = 16\n",
    "\n",
    "model_path = f\"Models/DDAMFNPP_{dataset_name}_best_model_LR_{lr}_BS_{batch_size}_P_{patience}_E_{num_epochs}_H_{num_head}_W_{workers}.pth\"\n",
    "\n",
    "# TestDDAMFN(data_directory, model_path, num_head, batch_size)\n",
    "TestDDAMFN(data_directory, model_path, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Happy\n",
      "Class probabilities:\n",
      "Angry: 0.0000\n",
      "Disgust: 0.0000\n",
      "Fear: 0.0000\n",
      "Happy: 1.0000\n",
      "Sad: 0.0000\n",
      "Surprise: 0.0000\n",
      "Neutral: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# def load_model(checkpoint_path, num_class=7, num_head=2, device='cuda:0'):\n",
    "#     # Initialize model\n",
    "#     model = DDAMNet(num_class=num_class, num_head=num_head)\n",
    "#     model = model.to(device)\n",
    "    \n",
    "#     # Load checkpoint\n",
    "#     if not os.path.exists(checkpoint_path):\n",
    "#         raise FileNotFoundError(f\"Checkpoint file {checkpoint_path} not found\")\n",
    "        \n",
    "#     checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     model.eval()\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# def process_image(image_path, model, device='cuda:0'):\n",
    "#     # Define transformations (match validation transforms)\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((112, 112)),\n",
    "#         transforms.Grayscale(num_output_channels=3),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                              std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "    \n",
    "#     # Load and preprocess image\n",
    "#     img = Image.open(image_path).convert('RGB')\n",
    "#     input_tensor = transform(img).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    \n",
    "#     # Make prediction\n",
    "#     with torch.no_grad():\n",
    "#         outputs, _, _ = model(input_tensor)\n",
    "#         probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "#         _, predicted_idx = torch.max(outputs, 1)\n",
    "    \n",
    "#     return {\n",
    "#         'class_index': predicted_idx.item(),\n",
    "#         'class_label': class_names[predicted_idx.item()],\n",
    "#         'probabilities': probabilities.cpu().numpy()\n",
    "#     }\n",
    "\n",
    "\n",
    "# # Configuration (modify as needed)\n",
    "# checkpoint_path = \"TrainedModels/DDAMFN_DatasetFER_ResEmoteNet_Batch32_LR0.0005_Epochs10.pth\"\n",
    "# image_path = \"C:\\MastersRepos\\Deep-Learning-For-Computer-Vision\\Models\\DDAMFN++\\Datasets\\FER_ResEmoteNet\\\\test\\\\test_28713_happy.jpg\"\n",
    "# # class_names = ['Happy', 'Surprised', 'Sad', 'Angry', 'Disgusted', 'Fear'] #JAFFE\n",
    "# class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral'] #FER\n",
    "\n",
    "# # Load model\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = load_model(checkpoint_path, num_class=7, device=device)\n",
    "\n",
    "# # Process image\n",
    "# result = process_image(image_path, model, device)\n",
    "\n",
    "# print(f\"Predicted class: {result['class_label']}\")\n",
    "# print(\"Class probabilities:\")\n",
    "# for cls, prob in zip(class_names, result['probabilities']):\n",
    "#     print(f\"{cls}: {prob:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
