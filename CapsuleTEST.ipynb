{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow numpy matplotlib scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load FER-2013 dataset\n",
    "def load_fer2013(data_path):\n",
    "    with open(data_path, 'r') as f:\n",
    "        lines = f.readlines()[1:]  # Skip header\n",
    "    images, labels = [], []\n",
    "    for line in lines:\n",
    "        emotion, pixels, usage = line.strip().split(',')\n",
    "        pixels = np.array(pixels.split(), dtype='float32') / 255.0  # Normalize to [0, 1]\n",
    "        images.append(pixels.reshape(48, 48, 1))  # Reshape to 48x48 grayscale\n",
    "        labels.append(int(emotion))\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load dataset\n",
    "data_path = 'fer2013.csv'  # Path to your FER-2013 CSV file\n",
    "images, labels = load_fer2013(data_path)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_images, val_images = images[:30000], images[30000:]\n",
    "train_labels, val_labels = labels[:30000], labels[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cbf7df85b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGeCAYAAADSRtWEAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAALt9JREFUeJzt3Q1sVfX9x/EfykOhSEt5aKk8CkxAxQdU7GTKkMmMYTDM5jKXMWc0KhqBZJsk02XLFsiW+LSBms1p9uAwLEGHi0yHilksKAiKT1UXkMpzC4VCEUTvP7+zfxlVzvdze37tflf6fiU32v56zj33d865X8693+/5dsrlcjkHAMD/2En/6ycEAMAjAAEAoiAAAQCiIAABAKIgAAEAoiAAAQCiIAABAKIgAAEAoiAAAQCi6OwKzCeffOK2bt3qTjnlFNepU6fYmwMAaCV/g53GxkZXWVnpTjrJuM7JtZPf/OY3uSFDhuS6deuWu/DCC3OrV6/Oa7na2lp/ayAePHjw4OE+3w//fm5plyugxx57zM2dO9c98MADbvz48e6ee+5xU6ZMcTU1Na5///7msv7Kx5s0aZLr3Lmz+TfHc/LJJ5vr91HZsmfPHpeVem7zXwLOuaamptSxLl26BD333r17U8fOO+88c9khQ4aY40VFReb4xx9/nDr20Ucfmcuq8YaGhtSxDz/80Fz24MGDLoT1uouLi81le/XqZY6XlZWljvXr189c1jo/vK5du5rjJSUl5icUlrRzttmRI0cyjXkbNmwwx3fu3Jn5/Dh06JA5vm/fPnO8rq4u0zGazzFuHafqdan3jQMHDpjj+/fvz3Qs+LGNGzfKY7FdAtBdd93lrr/+enfttdcmP/tA9Pe//939/ve/d7fffru5bPPHbv5ATps8a1JDd4g6gdozAFnPrbZLPbc1rt6QVIAJCUChc9atW7fMb5bWduXDmjdru/IZ7969e+pYjx49zGXVuHrunj17RglA6o3YmhP1ukLO63zOEet9RT13TtwP2lpenT/t+b6Rz1ck6m/aPAnh8OHDbu3atW7y5Mn/fZKTTkp+rq6uPu6/PPy/Lo59AABOfG0egPylqP+XZXl5eYvf+5+3b9/+mb+fP39+csnf/Bg0aFBbbxIAoABFT8OeN29e8v1E86O2tjb2JgEA/gfa/Dugvn37Jp8b7tixo8Xv/c8VFRXH/dxWfSYNADjxtHkA8l/WjRs3zq1YscJNnz796BeX/udbbrkl7/X4LyTTvpw73kd5+WbSqC/81BfTvXv3zvxlvP9+LOsXhip7T32Be8UVV6SOXXDBBUFfaqs5D/lyV7Gyl0K2K58ECOuLZ3UsqNdtJQKoZUPH1esO+VLbOr+sLFDv3HPPNcefe+651LEtW7YEHeNqTqzXpY6Fj8S5G/Ilv8r0VEkK1nFo7a98azjbJQvOp2DPnDnTnX/++e7CCy9M0rB9ul9zVhwAAO0SgK6++mq3a9cud+eddyZXK+ecc45bvnz5ZxITAAAdV7vdisd/3Naaj9wAAB1L9Cw4AEDHRAACAERBAAIARFFw7Ria7d69OzWl00r/s26k6Kmb46k0bCttUd38MuTGmtbNKb1jb310PD4RJGtarkqtDaHuzafSOa1tC113yHOHpHCrcZU6G7o/rdetyhjUnFmp8ep1qXN35MiRqWP+Rsgh+0OlM1s3+1Xp5Z0D7hWntkuVb6j0cyuF3NrufNOwuQICAERBAAIAREEAAgBEQQACAERBAAIAREEAAgBEQQACAERRsHVAPq8+rZbCyk1Xee379+8PGrfy6kPqK7xRo0aljk2ZMsVc9ni9lvKtSwmtG1HL+3YcWdetxlX9RlC/elHLY9VBhB4LIdulnlvVnVj7S9XJqee2en+F1MN4o0ePTh17+umnM9fgqbYEqh5H7euPxZxatVPq/Uq1gFGyHgvqNTXjCggAEAUBCAAQBQEIABAFAQgAEAUBCAAQBQEIABAFAQgAEEXB1gFZ+edW7rvqvaF69vTq1StzDYZa9qyzzjLHL7nkktSx8vLyzNulqJz9kDoftW2hdUAhy4bMmVq/qv0IqV9Sryukp5WaF7VsCPW6rHoYr1+/fqljlZWV5rJr1641x0899dTMvYr27NljLltk1DWGUusuLS3NXHtl1USqmq1mXAEBAKIgAAEAoiAAAQCiIAABAKIgAAEAoiAAAQCiIAABAKIo6H5AabUUVk8RVdvRp0+fzLUEqtZn+PDh5rJnn322OV5cXOzaq4Yi37z8LNScW3VCITVEquZF1eJYx1F7z1lMqkeMNedqf4TWVoXo3r176thpp51mLvvPf/7THLdqXrwhQ4akju3bt89ctpM4Tq1zWx3Dqu6xrq7OHC8pKWmX87oZV0AAgCgIQACAKAhAAIAoCEAAgCgIQACAKAhAAIAoCjYN22KlHvbv3z/zbdPzuT35F77whdSxoUOHZk4TVWm/KiVYjVupnuoW+ypNVI2HpOaqdbdnGnXIdoe0PFCvW81J6P60qPTa0OO0vQwbNiwoXXnbtm2ZSyhUm5ZGkeId0h6jd+/e5nh9fb05fvDgwXZ5v2rGFRAAIAoCEAAgCgIQACAKAhAAIAoCEAAgCgIQACAKAhAAIIqCrQPyue9p+e8fffRR6nJHjhwx16vy5lU7Bqudg7o1umqZYLUWUHUKqrajS5cumZdV1JxbNQFdu3Z1hUrVMlj7K7QeJmTd1r72ioqKgs6RENY5oI4jVYNkLV9RURE0J7t27cq8bVYtTT7vC9brUnOijgVV92htm6pfygdXQACAKAhAAIAoCEAAgCgIQACAKAhAAIAoCEAAgCgIQACAKAq2DsjnvqfVp1i57SovXtU4lJSUmONW3Yrq8aLqN6zlrbqQfOqErOVVrUBI7xpVxxBat2XVKYT2rlHzYi2v5kTVjFn7Sx0Las5UXUrIsaBel7W8Wtaq//MOHz6cuc5H9QnbunVr5noadW52C6gfDOlZlc+8WMeKde6qY7QZV0AAgCgIQACAKAhAAIAoCEAAgCgIQACAKAhAAIAoCjYN26fxZUnDVnr16mWOFxcXZx5Xt1UPuZ18aLqllRYZkuqcz/JWSrJKrQ1NI7U0NDQErdvaJ6Ep3tb4oUOHzGVVCniPHj0yH4fqdaljPN/03CzHWcixpM6vAwcOmONWqnXIMaqOhdDzR7Fel7Wv1XHSjCsgAEAUBCAAQBQEIABAFAQgAEAUBCAAQBQEIABAFAQgAEAUBVsH5G9Rnpabb92+vKyszFyvGu/du3dQLYJF5cZb42pZVfth1Tmo2g1Va6CWD1m2sbHRHG9qasp8i301p3v27DHHreNQ1eqo5y4vL8/8uqy2BPkcKyFUzZg1rmqEVOuOkO1Sc6bG22u7VR2QqidTr1vNuXWMW8dwu7VjeOGFF9zUqVNdZWVlciA//vjjn9moO++80w0YMMB1797dTZ482b377rutfRoAwAmu1QHIVwSfffbZbuHChccd/+Uvf+nuu+8+98ADD7jVq1cndw6YMmWKbMoEAOhYWv150hVXXJE8jsdf/dxzzz3uxz/+sZs2bVryuz/84Q/Jxwn+Sulb3/pW+BYDAE4IbZqEsHHjRrd9+/bkY7djW1yPHz/eVVdXp35Wvm/fvhYPAMCJr00DkA8+x/sC1f/cPPZp8+fPT4JU82PQoEFtuUkAgAIVPQ173rx5bu/evUcftbW1sTcJAPB5C0AVFRXJf3fs2NHi9/7n5rHjpfn5FgnHPgAAJ742rQMaNmxYEmhWrFjhzjnnnOR3/jsdnw130003tWpdvodGWu1Knz59UpcbOXKkud6BAwcG9UpRefUWlRsfsm5Vp2DVjvh0+ZDtCumbo6h6mYMHD2auxVHbFVJjoZa16pc8K2tU7Y/9+/eb4127ds18rKjjTGW7WjUxqiZM7S9r3aqWTe0PVf9n7W91fjWJ5w6p6VI1SCGvy9pf+dYBtToA+YP7vffea5F4sH79+qTAc/DgwW727Nnu5z//eRIIfEC64447kpqh6dOnt/apAAAnsFYHoDVr1rgvf/nLR3+eO3du8t+ZM2e6Rx55xP3whz9MaoVuuOGGpOPkhAkT3PLly4O7AgIAOngAmjhxovmxiL8k/NnPfpY8AAAo2Cw4AEDHRAACAERBAAIARFGw7Rh8Vl1auunQoUNTlzvttNPkei0qWSKkHYNKibTGVQqqSs210jVVyqQaV+mzVhpqSBsJz989IyuVmmvdij6f9FpLfX29Oe6LsrOml6t1q/1pnQOqFYRatzXnIWUI6rl3796deb7zeV+wUqnVMdwpoD2GKlNQ+0Ol5Fvrt16X2q6j68jrrwAAaGMEIABAFAQgAEAUBCAAQBQEIABAFAQgAEAUBCAAQBQFWwfku6im1dwMGTIkdbl+/foF5b2rOh8r913Vw6icfOtW9iqvXr2uT/doas3t+62WB/nU01h1Dn379g1qj2HVAaneUmrO1HNb61ftGFRtSGNjY+qYaluvxtW8bNmyJXXMaoXi9ezZM3PtlKoDUq0erOWt+cxnX4e0oVDHQmfxnmOd+/m2Pciy7rZYv8IVEAAgCgIQACAKAhAAIAoCEAAgCgIQACAKAhAAIAoCEAAgioKtA/J9e9Ly53v37p25h4vKyVesnH1VB6T6uFj1NKG1OlaPGNULRdXLqH4mVi1BXV1d5mVVfYfaLlV/MXDgQHPcqjlTfXNKS0vNcWvbVe2Gqqexeteo3jlq3SH9gFQNkXrd1pyF1nypc9eqEwrpIab6hIXMSSjrdeX7vFwBAQCiIAABAKIgAAEAoiAAAQCiIAABAKIgAAEAoiAAAQCiKNg6IJ+Xn5a7b/UUUX1WVB1DSN68Wrdi9RRRtTqqtsOqUbLmM7S2Q403NDSYyx44cMAct+qIVP+YoqIic/yDDz7IvLzqm2PVsnmDBg3KvKzqiaX2l9VPSNWjqTo8q15G1eqoOqGQGr/i4mJzfM+ePZlrdayx0H5ASsiy6v0wpDfa0XVk2ioAAAIRgAAAURCAAABREIAAAFEQgAAAURCAAABRFGwatk+pTEurtFIDrTTPfKhUUCtlUqWAqzRRa90qZVilHFvpyhs3bgxKTVepniqFNSS9XM1L1nTjfNJnS0pKMu8PlV7eq1evzK0eVAqsSn3ftm1b5rYEqlygtrY283GkXrfVPkOlOqvx9izP6BTYNiSkhEI9t3V+qfMjH1wBAQCiIAABAKIgAAEAoiAAAQCiIAABAKIgAAEAoiAAAQCiKNg6IH/L+LQcdSv/XNUSqBqJkHYOallVx2Atr2oJVL6/dYt+lc+vblVv1RiptgZqf6jb+0+aNClzfVN9fX3QnFrjpaWlQXNq1aOp9hlq3Zs2bTLHly9fnnnOVN2WdYyrepdzzz038+tWx4J631DbZrW4UMfRSeJ9wxpXdTzq/App12C9b6jX3IwrIABAFAQgAEAUBCAAQBQEIABAFAQgAEAUBCAAQBQEIABAFJ/LOiCr1qB3795BOfchvTlUzUpIrYHqJdS3b19z3Kodef/9981lVY8ltW1WLcLmzZvNZauqqszxCRMmZO7tpGpDVD2NVftx6qmnmsuWlZWZ4wMGDEgd69+/f9Cx8Pjjj5vj69evTx07ePBg5nozb+LEiS6rnTt3Zq5L2b17d9C5qerwrGNB1dl1EedPyHYp6nW3Rc8fC1dAAIAoCEAAgCgIQACAKAhAAIAoCEAAgCgIQACAKAo2DbuxsTE1PfHQoUNR2i2E3iZf3dLduoW5auWg0mOtdMvKykpz2ddff90cV6/7G9/4Rubt7tOnT+Y5GzZsmLnswIEDzfGSkpLMbShCWyZYr1uleKs0bZWePmjQoNSxhoYGF2L48OGpY+ecc4657MqVKzOnQu/fv99cVpVQqPKMkHTokwOWVWnUal+HtHOwtjvfNg9cAQEAoiAAAQCiIAABAKIgAAEAoiAAAQCiIAABAKIgAAEAoijYOiCft59WN2PVfli1APnUCan8dStvXtX5hNQaqO1WtQQffvhh5hqjM844wxzftGmTOb5t27bUsZ49e5rL1tfXZ76VvarjUbU4an9ZrT/UcaTWbbVr6NGjh7lsr169zPGvfvWrmds5qJYI6hy49NJLXVZTpkwxx995551Mx38+x4I6v6z3pPbUSdTxqFYPIe93WccyXwHNnz/fXXDBBckbli92mz59uqupqfnMjp41a1ZSSOffXK666iq3Y8eO1jwNAKADaFUA8pXIPrisWrXKPfPMM8nVxuWXX+4OHDhw9G/mzJnjli1b5pYsWZL8/datW92MGTPaY9sBAB3lI7jly5e3+PmRRx5JroTWrl3rLrnkErd371730EMPuUcffdRNmjQp+ZuHH37YjR49OglaF110UdtuPQCgYyYh+IBz7OfVPhD5q6LJkycf/ZtRo0a5wYMHu+rq6tT7uu3bt6/FAwBw4sscgPyX4rNnz3YXX3yxO/PMM5Pfbd++Pbn5XWlpaYu/LS8vT8bSvlfyXxY3P6wbIQIAThyZA5D/LsjfJXnx4sVBGzBv3rzkSqr5UVtbG7Q+AMAJnIZ9yy23uCeffNK98MILLW5pX1FR4Q4fPpzcsv3YqyCfBefH0tJRVUoqAKCDByCfM37rrbe6pUuXuueff/4z/VbGjRuX5J2vWLEiSb/2fJr25s2bXVVVVas2zNeOpOXeH5t119b5+iF1QqG1OlbOvlpWBXGrH4qaE6veJZ9+Qr63U5ojR46Yy4b0d1K9n1Tth6ppsXoZWT2r8tlfVj8htayq/RgzZkzm16V6KKm6rj179mTuoaT6O61bt67d6gPzrWvJchwpITVG6nWrYyVrbWK+/YA6t/ZjN5/h9sQTTyS1QM3f6/jvbvzB4/973XXXublz5yaJCb4gzgcsH3zIgAMAZA5A999/f/LfiRMntvi9T7X+3ve+l/z/3XffnfzL018B+X8F+urlRYsWteZpAAAdQKs/glOKiorcwoULkwcAAGm4GSkAIAoCEAAgCgIQACAKAhAAIIqC7Qd06qmnpubPW/UCKlFC1WeoWgRreZVTr2p5rHoBVdOiesRYy6taHPW6VA2F1ZdH1SmE1ED4hJiQ7VbzYm2b2tdq23xBd5bnzedYUf2CTjvttNSxLVu2mMuG3MvR6kOUTz2a9dxqX6p+QWp/tlfPHSW0v1nI67LmNN/zlisgAEAUBCAAQBQEIABAFAQgAEAUBCAAQBQEIABAFAWbhu1vve67q7Y2VVqlU6r0V6ttgboVvkr1tG5zr1IiVTpl2lzlk+KttlulSqtts5ZX6ZrqVvbWeEh6az7zYu0v9bpC0s+tFO22mFN/p/s0aX29mg0dOtQct/ZJSJmC2l/qWFD7WrHOgZBSAkWtW71utby1T6x0/3zbMXAFBACIggAEAIiCAAQAiIIABACIggAEAIiCAAQAiIIABACIomDrgHxeflqeuVXro2ptQnPyrfWrOoWQ51a3XVdtJqx2DVZtUz7PrWparFoE1eohREg7hXxqyqzlQ+fM2idqu1WdkGLVlFk1Qt6BAwcyr1u1QlHHirW/VY2ROlZUPY01Hlpn11m8r1jUcajWbR1LVq0PdUAAgIJGAAIAREEAAgBEQQACAERBAAIAREEAAgBEQQACAERRsHVAvg4irWbAyjFXOfX5PG/WOiBVa6DqHNoirz5LfYZVI5RP/YXqRaTmtD3rM0J6PzU2NmZ+blV/obbbqs9Qx7iqhVPHYcixoI4lq4eMOo5C+hypepfQOruQYyEnzm1rzkKp5w7p35QProAAAFEQgAAAURCAAABREIAAAFEQgAAAURCAAABRFGwatk+bzHKrfpWqGZoea6WKqpRhlcoZknqrUjWtbVPbXVxcHDSn1vKhqZzWtjc1NZnL7t27N2jcateg2haocetYKCoqMpdVc6rSnbNuVz7jIcewOgesYyEk7T2fczfk/Aih3u/UsRCyvPX+nG95BFdAAIAoCEAAgCgIQACAKAhAAIAoCEAAgCgIQACAKAhAAIAoCrYOyGLlmKuaFlVbdPjw4cy3ulf1GSo3PqTWQNUxWLUfartDajtUrYG6HbyqobD2t1Wn4zU0NGRuYeGVlZW1W9sCq2WCquMJHbeOBzWn6nVbx7iab3VuWu01QuuXVEsR67lD20zkjHMktI5OLW/VXlnbrV5TM66AAABREIAAAFEQgAAAURCAAABREIAAAFEQgAAAURCAAABRFGwdkO8NktYfJKS/hspPV+NWHVDPnj3NZVU9gLXu0J49Vg2SqoFQfVrU8lb9hqpDUPvaWndjY2NQHZBV56PG1XarObP2V0jvp3yOcetYsupd8nld1rapfj+7du0yx+vq6jJvl6LqgELqcXKiFi6ktlAdC2q7rXmztjvf18QVEAAgCgIQACAKAhAAIAoCEAAgCgIQACAKAhAAIAoCEAAgioKtA/L56Wk56lY9jcrXV/UZ7dl/RvVKsfqwhGyXqhew6o/yWXdo/5mQ57Z6yNTX15vLqrqTkN5R6jWr5w45zlSdkKrlUfVsIa/LqkFSNS0bN240x5uamjLvD3V+qWPBGld9jD4KOBZC65vUnGddNt/1cgUEAIiCAAQAiIIABACIggAEAIiCAAQAiIIABACI4nPZjsFKM1XplKptQUhKskqPtdKsVUpljx49gm6xb6VFhtwOPh8h61cprFYatkpvVe0atmzZkvlY2bNnj7msOlZOOeWU1LHRo0eby/bv3z/znKlShgEDBgSlcB86dCjzcfLWW29lPjdDSgFCyzdCSygOi3MgRHuf+216BXT//fe7sWPHul69eiWPqqoq99RTT7U4qWbNmuX69OmTHIhXXXWV27FjR3tsNwDgc65VAWjgwIFuwYIFbu3atW7NmjVu0qRJbtq0ae6NN95IxufMmeOWLVvmlixZ4lauXOm2bt3qZsyY0V7bDgDoKB/BTZ06tcXPv/jFL5KrolWrViXB6aGHHnKPPvpoEpi8hx9+OPm4wI9fdNFFbbvlAICOmYTgv3NYvHhx8nmy/yjOXxX5z9wnT5589G9GjRrlBg8e7Kqrq83PhPft29fiAQA48bU6AG3YsCH5fsd/UXnjjTe6pUuXujFjxrjt27cnX/SVlpa2+Pvy8vJkLM38+fNdSUnJ0cegQYOyvRIAwIkdgE4//XS3fv16t3r1anfTTTe5mTNnujfffDPzBsybN8/t3bv36KO2tjbzugAAJ3Aatr/KGTFiRPL/48aNcy+//LK799573dVXX52kCzY0NLS4CvJZcBUVFanr81dS6g7WAIATT3AdkM9x99/j+GDkb0m+YsWKJP3aq6mpcZs3b06+I2rLdgxWTUtozr267bpVW6Ly9VXth1UHpOp81C341eu2qFurq+e2ag3UutWcWfNSVlZmLvvOO++Y4y+++KI5vmnTptQx62PnfOovfFJPmldffTVof6iamEsvvTTTdoW2Y9i1a5e57Lvvvpv53FXvC2p/hJwDqmVCJ7Fude6HCKkfTHt/bk19UefWflx2xRVXJIkFvojPZ7w9//zz7h//+Efy/c11113n5s6dm5z4vk7o1ltvTYIPGXAAgKAAtHPnTvfd737Xbdu2LQk4vijVB5+vfOUryfjdd9+d/EvAXwH5q6IpU6a4RYsWteYpAAAdRKsCkK/zUbeaWbhwYfIAAMDCzUgBAFEQgAAAURCAAABREIAAAFEUbD8gX/+Rlktu9dVROfWqTkHVUIRQtTjWuNpuVcxr5eWrnH1VK6DGrfWH1mdYfXPUuocPH26Oq7oUq76juVg7a08eX9CdtWfV0KFDzfEvfelL5rhV66OO4ZA6INXvx5oTtT9UPZlV05JPfaC1vDoOPxbnj/W61LpDtef7YbL+dl07AAApCEAAgCgIQACAKAhAAIAoCEAAgCgIQACAKAo2Ddune6alNobcnlylFapxla4ZwkpxVa0e1HZZqZwqdT00TdtKFVXrVunl3bt3z7xdp556qjk+ceLEzGnaKlXa303eUldXl7nNxIQJE8zx0aNHm+NbtmzJnIatjkMr/fy1115zIUJajoSyzqGQlgeh61b7I7TNSyiugAAAURCAAABREIAAAFEQgAAAURCAAABREIAAAFEQgAAAURRsHVBTU1NqXr+Vm15cXBx0u3h12/WQmhVVy5O1jieffH2rlkDV4qg6hZD6C/W61O3mDx06lLnGQW23Wt6qQerVq5e5bHl5uTnev3//zOtWc6bOAet1qTlT509NTU3q2HvvvWcua7VhUa879BgOacegzq+T27G2MKTVw/8CV0AAgCgIQACAKAhAAIAoCEAAgCgIQACAKAhAAIAoCEAAgCgKtg4oa85+aL+SkL47qv5C1epYy6v+MSHPrWoBVA2EqjWwanVU7dS+ffvMcavvTsh8N9eiZX3d6jjq2rWrOW7tE7W/PvzwQ3O8vr7eHO/Zs2fm16X6IL344ovt1s9H1TdZVK1OCHUcdRXHgjWu5kwdK6o+ymKdX/n2EeIKCAAQBQEIABAFAQgAEAUBCAAQBQEIABAFAQgAEEXBpmH71N201EgrxU+lYqq0X5USaaXuqudWqYlWWm/o7eKt163WrbZbvW5r/VaKtrd3797M6bMqxVSNq/RyK11ZHUcqndmac6tdQj6pt7t37868fI8ePcxlV69ebY5v2rQp8+tSx4p1HFr7Kp91K8OGDUsdGz16tLns+++/b45v3rw5deyUU04Jer8LLR0JxRUQACAKAhAAIAoCEAAgCgIQACAKAhAAIAoCEAAgCgIQACCKgq0D8reUV3UYWYTmvVvj6jb46rmtnH1Vs6Keu6ioKPOt6NV2q/1k3Y5e3b6/PW8Jr2qnVH3T/v37M823V1lZaY5b+1vNt9qf6liyllc1Y9XV1ZnnVNUvqVYq1nZb+8obMWKEOT527FhzvF+/fqlj3/72t4P21zPPPJM6tmjRInPZbdu2meNqzrPKt7UGV0AAgCgIQACAKAhAAIAoCEAAgCgIQACAKAhAAIAoCEAAgCgKtg7owIEDsk4jS/2F6vuhag2sGgxVd6J6xIT0tlF591a9jeoZouoUVF2KtW11dXXmsrt27TLHGxsbU8f27dtnLtvQ0NBufY5UfcXIkSPNcetYUsdwVVWVOd6nT5/M4x988EHmfj+q5486htX+qKioSB07++yzzWWvv/56c/y1114zx//4xz+mjk2YMMFctlevXub4RRddlDpWVlZmLnvPPfeY4+ocsd6zrNpDta+acQUEAIiCAAQAiIIABACIggAEAIiCAAQAiIIABACIggAEAIiiYOuAfP+OtFoKq15A5Z/37NkzqDbE6o2j6nzUuFX7Edq3w5oXNWeqNkr1IrJqmAYMGGAuq/q4vP/++6lje/bsCVq31cdIjffo0cNcVtXTlJaWpo6VlJRk7k3jFRcXZ97f69atk/V7WZ9bnZvf/OY3zfFLLrkk85yofW3Vm6nzc/78+eay27dvz1w7pV7XoEGDzPEtW7ZkHu/du3fm94xmXAEBAKIgAAEAoiAAAQCiIAABAKIgAAEAoiAAAQCiKNg0bCvV2kqZVOnKKt1StVSwUor37t2bednQFG+1bmv50BYVyimnnJL5VvQjRowwxydOnJh5u9S+VinFVsq+Og7V67ZSrVX7C9U+Q70uKz391VdfNZdVLVSsY/yyyy4zlx0/fnzm/fG3v/3NXFa9rp07d5rjhw8fzpzCXV9fnzklX73nqJR769xUx9I777yT+RhtkyugBQsWJG98s2fPblETMmvWrKSniM/rv+qqq9yOHTtCngYAcALKHIBefvll9+CDD7qxY8e2+P2cOXPcsmXL3JIlS9zKlSvd1q1b3YwZM9piWwEAHT0A+Uv0a665xv32t79tUQ3rLwcfeughd9ddd7lJkya5cePGuYcffti9+OKLbtWqVW253QCAjhiA/EdsV155pZs8eXKL369duza5rcuxvx81apQbPHiwq66uTv3+wbeFPfYBADjxtToJYfHixe6VV15JPoI73j2N/Jfdn/7SrLy8PPV+R/4+ST/96U9buxkAgI50BVRbW+tuu+029+c//9kVFRW1yQbMmzcv+eiu+eGfAwBw4mtVAPIfsfl0xPPOOy9JM/UPn2hw3333Jf/vr3R8OuKn0yF9FlxFRcVx19mtW7ckJfXYBwDgxNeqj+B8nv6GDRta/O7aa69Nvuf50Y9+lNz629cBrFixIkm/9mpqatzmzZtdVVVVq+uA0nLQrbx6VdOibl+uruysOgZF1fJYVF2JWrdVy6PaKag5sW7LrmpDVCuIgwcPZp4XVQ9j3eY+n1vZjxw5MnXM/8Mq5DhStTohx4JqU2HVd/jzOeQ4Peuss1LHTjvttKDtfuuttzK3v1C1OOocKSsrc1l1EjV81vudOsbVuq0aI+9rX/tapjYS/n347bffdm0agHzR0plnnvmZQidf89P8++uuu87NnTs32SH+aubWW29Ngs9FF13UmqcCAJzg2vxOCHfffXdSYe6vgHwUnDJlilu0aFFbPw0AoKMHoOeff/4zH9csXLgweQAAkIabkQIAoiAAAQCiIAABAKIgAAEAoijYfkBZ7d69O6gHjMqLt/LuVf2Fb09hUTUxIfUXVi2BqvNRc6a2W/VDydoXSr1u1cdI3XfQ38k9a42Sem51nPXt2zdz/ZKqWVHnSNp9Gz1fbB7yui644ILMvWnUnKqePpYxY8aY4++9917muq20Ivx8e/pY7zlWD6R86iLV+XXGGWdk6tWl6veacQUEAIiCAAQAiIIABACIggAEAIiCAAQAiIIABACIomDTsH1aY1rapXWre5UyHJqmbaUsq9RDtW6rF5JK8W5qajLHreXVdvk7nlvU6/74448zp9aqlGLrFv3q9v2+d1XI+JYtWzKnx1qtHLwvfvGLmVOh1f586aWXzHHf8TjNp++G/2lTp041x62UZHUsqOPMt33JWiqgWopUVlZmTunftWuXuWy52J9W+YbVOiOf80eVIvh+b1naZ6j072ZcAQEAoiAAAQCiIAABAKIgAAEAoiAAAQCiIAABAKIgAAEAoijYOiBfJ9GlS5dW33a9R48e5npVbYi6PXn//v0z1bvks26rVkHdgl/VUFi3uk+b53xbPShWXYp6bjWnVr2Bus29qpFQc2rtE6tWTd1i39u+fbvLSh1nTz75ZOZ6mptvvtlc9qyzzspcB6T29b///e/M549qrbFjxw5zXLVUsGpi1DG+Qzz30KFDU8dKSkrMZWtqaoLObescWr9+feZjsBlXQACAKAhAAIAoCEAAgCgIQACAKAhAAIAoCEAAgCgIQACAKAq2DsjX26T1sLF6qVg1DPnUZ6h+Qb5PUdacfNXTx6o76dSpU+Y+RapGQvW9UX1YrD5GatvUslYvFK+0tDR17PTTTzeXVfUZn3zySeY5VT15VI2RRdXLPPfcc+Z4bW2tOf6DH/wgdew73/mOuazqu2Mdx3V1deayqhZu2LBhqWOvvfaauezEiRPN8aqqKnN8wIABmXtDrVu3LvP+GjJkiLms6t+kaqusuknrdaljtBlXQACAKAhAAIAoCEAAgCgIQACAKAhAAIAoCEAAgCgIQACAKAq2Dsjnn6fVaVg9ecrKysz17t+/3xyvr6/PnDe/bdu2oBqkpqamzLU4qgZJ1bS013ar5VVtR3l5eeZaHtVzx+qRlM+cWbUOqg5C1cs0NjZmrmlZvXp1UA8Yq6ZF9SkqLi7OfH5t2rQp87LemDFjUseGDx9uLtu7d++gPkcDBw7MVDuYD6umTNUYWXVyas7UcWy9l6q6xWZcAQEAoiAAAQCiIAABAKIgAAEAoiAAAQCiIAABAKIo2DTsrCmqKh3ZSuHOZ9y65fuuXbvMZd977z1z3Fr+0KFD5rL79u3L3BLBSrvNJw1bpTNbLS5UuqZq12Atr1KdVTqy2jarjYVaVu1PK8V1w4YN5rIffPBB5nV7f/rTnzKfX4MHD858LKl2JWpOreNYpdSrc1e1abHSz1Vrjk/EtlnzMmLECHPZLVu2ZF63N27cuEwp3v7ce+utt5zCFRAAIAoCEAAgCgIQACAKAhAAIAoCEAAgCgIQACCKgkvDbr6DsZVCa41ZqbGhd4VWz33kyJGg57bu3qyWDblzs9puNacqpfjkk0/OfJdvdadta87U61JpvTHTsK3XrdLL1V3AFWve1P5QKd7Wtqu7Rqtjxbozujo/1LrVtlmlISoN+6B4butY+fDDD9v13LXmLZ/3aHUsdsqFHq1tzNcwDBo0KPZmAAAC1dbWmq0qCi4A+Yi7devWpLjR/yvSF1j6gORfiCpKxH8wZ63HnLUec9Z6HWXOcrlcclVYWVlpXgEW3EdwfmOPFzH9zjqRd1h7YM5ajzlrPeas9TrCnJWIu2Z4JCEAAKIgAAEAoij4AORvXviTn/xE3hAT/8WctR5z1nrMWesxZwWehAAA6BgK/goIAHBiIgABAKIgAAEAoiAAAQCiIAABAKIo+AC0cOFCN3ToUFdUVOTGjx/vXnrppdibVDBeeOEFN3Xq1OR2F/62RY8//niLcZ/geOedd7oBAwa47t27u8mTJ7t3333XdVTz5893F1xwQXKbp/79+7vp06e7mpqaz9zccdasWa5Pnz6uZ8+e7qqrrnI7duxwHdn999/vxo4de7R6v6qqyj311FNHx5kz24IFC5Lzc/bs2Ud/x5x9DgLQY4895ubOnZvkzb/yyivu7LPPdlOmTHE7d+6MvWkFwd+h18+JD9LH88tf/tLdd9997oEHHnCrV692xcXFyfypO+ieqFauXJmc9KtWrXLPPPNMcsfeyy+/vMWdjufMmeOWLVvmlixZkvy9vy/hjBkzXEfmb43l30TXrl3r1qxZ4yZNmuSmTZvm3njjjWScOUv38ssvuwcffDAJ4Mdizv5froBdeOGFuVmzZh39+eOPP85VVlbm5s+fH3W7CpHflUuXLj368yeffJKrqKjI/epXvzr6u4aGhly3bt1yf/nLXyJtZWHZuXNnMm8rV648Oj9dunTJLVmy5OjfvPXWW8nfVFdXR9zSwtO7d+/c7373O+bM0NjYmBs5cmTumWeeyV166aW52267Lfk9c/ZfBXsF5PtY+H9x+Y+Njr1Rqf+5uro66rZ9HmzcuNFt3769xfz5mwP6jzGZv//Yu3dv8t+ysrLkv/5481dFx87ZqFGj3ODBg5mzY/pKLV68OLlq9B/FMWfp/NX2lVde2WJuPOasgO+G3ayuri452MvLy1v83v/89ttvR9uuzwsffLzjzV/zWEfm2374z+Qvvvhid+aZZya/8/PStWtXV1pa2uJvmTPnNmzYkAQc//Gt/85i6dKlbsyYMW79+vXM2XH4IO2/NvAfwX0ax9nnIAAB7f2v09dff93961//ir0pnwunn356Emz8VeNf//pXN3PmzOS7C3yW7/Vz2223Jd8z+uQppCvYj+D69u2btHL+dGaI/7mioiLadn1eNM8R8/dZt9xyi3vyySfdc88916L3lJ8X/9FvQ0NDi79nzlzyL/YRI0a4cePGJdmEPvnl3nvvZc6Ow3/E5hOlzjvvPNe5c+fk4YO1Twjy/++vdJizAg9A/oD3B/uKFStafGzif/YfBcA2bNiw5GA+dv58N0afDddR58/navjg4z8+evbZZ5M5OpY/3rp06dJiznya9ubNmzvsnKXx5+KhQ4eYs+O47LLLko8s/RVj8+P8889311xzzdH/Z87+X66ALV68OMnaeuSRR3Jvvvlm7oYbbsiVlpbmtm/fHnvTCibLZt26dcnD78q77ror+f/3338/GV+wYEEyX0888UTutddey02bNi03bNiw3MGDB3Md0U033ZQrKSnJPf/887lt27YdfTQ1NR39mxtvvDE3ePDg3LPPPptbs2ZNrqqqKnl0ZLfffnuSKbhx48bkOPI/d+rUKff0008n48yZdmwWnMec/UdBByDv17/+dbKjunbtmqRlr1q1KvYmFYznnnsuCTyffsycOfNoKvYdd9yRKy8vTwL5ZZddlqupqcl1VMebK/94+OGHj/6ND84333xzkmbco0eP3Ne//vUkSHVk3//+93NDhgxJzsF+/folx1Fz8PGYs9YHIObsP+gHBACIomC/AwIAnNgIQACAKAhAAIAoCEAAgCgIQACAKAhAAIAoCEAAgCgIQACAKAhAAIAoCEAAgCgIQAAAF8P/AQBxk2nE5v3cAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(train_images[0].reshape(48, 48), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import layers, models\n",
    "\n",
    "# class CapsuleLayer(layers.Layer):\n",
    "#     def __init__(self, num_capsules, dim_capsule, routings=3, **kwargs):\n",
    "#         super(CapsuleLayer, self).__init__(**kwargs)\n",
    "#         self.num_capsules = num_capsules\n",
    "#         self.dim_capsule = dim_capsule\n",
    "#         self.routings = routings\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.input_num_capsules = input_shape[1]\n",
    "#         self.input_dim_capsule = input_shape[2]\n",
    "#         self.W = self.add_weight(shape=[self.input_num_capsules, self.num_capsules, self.input_dim_capsule, self.dim_capsule],\n",
    "#                                  initializer='glorot_uniform', trainable=True)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         inputs_expanded = tf.expand_dims(inputs, 2)\n",
    "#         inputs_tiled = tf.tile(inputs_expanded, [1, 1, self.num_capsules, 1, 1])\n",
    "#         inputs_hat = tf.matmul(inputs_tiled, self.W)\n",
    "\n",
    "#         b = tf.zeros(shape=[tf.shape(inputs_hat)[0], self.input_num_capsules, self.num_capsules, 1, 1])\n",
    "#         for i in range(self.routings):\n",
    "#             c = tf.nn.softmax(b, axis=2)\n",
    "#             outputs = tf.reduce_sum(c * inputs_hat, axis=1, keepdims=True)\n",
    "#             if i < self.routings - 1:\n",
    "#                 b += tf.reduce_sum(inputs_hat * outputs, axis=-1, keepdims=True)\n",
    "#         return tf.squeeze(outputs, axis=1)\n",
    "\n",
    "# def build_capsnet(input_shape, num_classes):\n",
    "#     inputs = layers.Input(shape=input_shape)\n",
    "#     x = layers.Conv2D(256, (9, 9), activation='relu')(inputs)\n",
    "#     x = layers.Reshape((-1, 8))(x)  # Primary capsules\n",
    "#     x = CapsuleLayer(num_capsules=32, dim_capsule=8)(x)  # Capsule layer\n",
    "#     x = layers.Lambda(lambda z: tf.sqrt(tf.reduce_sum(tf.square(z), axis=-1)))(x)  # Length of capsules\n",
    "#     outputs = layers.Dense(num_classes, activation='softmax')(x)  # Classification layer\n",
    "#     model = models.Model(inputs, outputs)\n",
    "#     return model\n",
    "\n",
    "# # Build CapsNet model\n",
    "# input_shape = (48, 48, 1)  # FER-2013 image size\n",
    "# num_classes = 7  # 7 emotions\n",
    "# capsnet = build_capsnet(input_shape, num_classes)\n",
    "# capsnet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# capsnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,992</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">51200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ capsule_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CapsuleLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)          │   <span style=\"color: #00af00; text-decoration-color: #00af00\">104,857,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">231</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m20,992\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_5 (\u001b[38;5;33mReshape\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m51200\u001b[0m, \u001b[38;5;34m8\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ capsule_layer_5 (\u001b[38;5;33mCapsuleLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m8\u001b[0m)          │   \u001b[38;5;34m104,857,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m231\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,878,823</span> (400.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,878,823\u001b[0m (400.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,878,823</span> (400.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,878,823\u001b[0m (400.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    def __init__(self, num_capsules, dim_capsule, routings=3, **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsules = num_capsules\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_num_capsules = input_shape[1]  # Number of input capsules\n",
    "        self.input_dim_capsule = input_shape[2]   # Dimension of input capsules\n",
    "        # Weight matrix: [input_num_capsules, num_capsules, input_dim_capsule, dim_capsule]\n",
    "        self.W = self.add_weight(shape=[self.input_num_capsules, self.num_capsules, self.input_dim_capsule, self.dim_capsule],\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs_expanded = tf.expand_dims(inputs, axis=2)\n",
    "        inputs_tiled = tf.tile(inputs_expanded, [1, 1, self.num_capsules, 1])\n",
    "        inputs_hat = tf.einsum('bijc,jcdk->bijdk', inputs_tiled, self.W)  # Fixed multiplication\n",
    "\n",
    "        # Routing algorithm\n",
    "        b = tf.zeros(shape=[tf.shape(inputs_hat)[0], self.input_num_capsules, self.num_capsules, 1])\n",
    "        for i in range(self.routings):\n",
    "            c = tf.nn.softmax(b, axis=2)\n",
    "            outputs = tf.reduce_sum(c * inputs_hat, axis=1, keepdims=True)\n",
    "            if i < self.routings - 1:\n",
    "                b += tf.reduce_sum(inputs_hat * outputs, axis=-1, keepdims=True)\n",
    "        \n",
    "        return tf.squeeze(outputs, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.num_capsules, self.dim_capsule)\n",
    "\n",
    "def build_capsnet(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(256, (9, 9), activation='relu')(inputs)\n",
    "    \n",
    "    num_primary_caps = ((input_shape[0] - 9 + 1) * (input_shape[1] - 9 + 1) * 256) // 8\n",
    "    x = layers.Reshape((num_primary_caps, 8))(x)  # Fixed capsule reshaping\n",
    "    \n",
    "    x = CapsuleLayer(num_capsules=32, dim_capsule=8)(x)\n",
    "    x = layers.Lambda(lambda z: tf.sqrt(tf.reduce_sum(tf.square(z), axis=-1)))(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Build CapsNet model\n",
    "input_shape = (48, 48, 1)\n",
    "num_classes = 7\n",
    "capsnet = build_capsnet(input_shape, num_classes)\n",
    "capsnet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "capsnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data augmentation\n",
    "# datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "# datagen.fit(train_images)\n",
    "\n",
    "# # Train the model\n",
    "# batch_size = 32\n",
    "# epochs = 20\n",
    "# history = capsnet.fit(datagen.flow(train_images, train_labels, batch_size=batch_size), validation_data=(val_images, val_labels), epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,992</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">51200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ capsule_layer_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CapsuleLayer</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1638400</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)       │   <span style=\"color: #00af00; text-decoration-color: #00af00\">104,857,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1638400</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1638400</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">231</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_12 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m20,992\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_12 (\u001b[38;5;33mReshape\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m51200\u001b[0m, \u001b[38;5;34m8\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ capsule_layer_12 (\u001b[38;5;33mCapsuleLayer\u001b[0m) │ (\u001b[38;5;34m1638400\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m8\u001b[0m)       │   \u001b[38;5;34m104,857,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda_7 (\u001b[38;5;33mLambda\u001b[0m)               │ (\u001b[38;5;34m1638400\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m1638400\u001b[0m, \u001b[38;5;34m7\u001b[0m)           │           \u001b[38;5;34m231\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,878,823</span> (400.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m104,878,823\u001b[0m (400.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">104,878,823</span> (400.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m104,878,823\u001b[0m (400.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    def __init__(self, num_capsules, dim_capsule, routings=3, **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsules = num_capsules\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_num_capsules = input_shape[1]  # Number of input capsules\n",
    "        self.input_dim_capsule = input_shape[2]   # Dimension of input capsules\n",
    "        # Weight matrix: [input_num_capsules, num_capsules, input_dim_capsule, dim_capsule]\n",
    "        self.W = self.add_weight(shape=[self.input_num_capsules, self.num_capsules, self.input_dim_capsule, self.dim_capsule],\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Expand inputs to [batch_size, input_num_capsules, 1, input_dim_capsule]\n",
    "        inputs_expanded = tf.expand_dims(inputs, axis=2)\n",
    "        # Tile inputs to [batch_size, input_num_capsules, num_capsules, input_dim_capsule]\n",
    "        inputs_tiled = tf.tile(inputs_expanded, [1, 1, self.num_capsules, 1])\n",
    "        # Reshape inputs_tiled to [batch_size, input_num_capsules * num_capsules, input_dim_capsule]\n",
    "        inputs_tiled = tf.reshape(inputs_tiled, [-1, self.input_num_capsules * self.num_capsules, self.input_dim_capsule])\n",
    "        # Reshape W to [input_num_capsules * num_capsules, input_dim_capsule, dim_capsule]\n",
    "        W_reshaped = tf.reshape(self.W, [self.input_num_capsules * self.num_capsules, self.input_dim_capsule, self.dim_capsule])\n",
    "        # Compute inputs_hat = inputs_tiled * W_reshaped\n",
    "        inputs_hat = tf.matmul(inputs_tiled, W_reshaped)  # Shape: [batch_size, input_num_capsules * num_capsules, dim_capsule]\n",
    "        # Reshape inputs_hat back to [batch_size, input_num_capsules, num_capsules, dim_capsule]\n",
    "        inputs_hat = tf.reshape(inputs_hat, [-1, self.input_num_capsules, self.num_capsules, self.dim_capsule])\n",
    "\n",
    "        # Routing algorithm\n",
    "        b = tf.zeros(shape=[tf.shape(inputs_hat)[0], self.input_num_capsules, self.num_capsules, 1])\n",
    "        for i in range(self.routings):\n",
    "            c = tf.nn.softmax(b, axis=2)  # Shape: [batch_size, input_num_capsules, num_capsules, 1]\n",
    "            outputs = tf.reduce_sum(c * inputs_hat, axis=1, keepdims=True)  # Shape: [batch_size, 1, num_capsules, dim_capsule]\n",
    "            if i < self.routings - 1:\n",
    "                b += tf.reduce_sum(inputs_hat * outputs, axis=-1, keepdims=True)  # Update b\n",
    "        return tf.squeeze(outputs, axis=1)  # Shape: [batch_size, num_capsules, dim_capsule]\n",
    "\n",
    "def build_capsnet(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Convolutional layer\n",
    "    x = layers.Conv2D(256, (9, 9), activation='relu')(inputs)\n",
    "    # Reshape to [batch_size, num_input_capsules, dim_input_capsule]\n",
    "    x = layers.Reshape((-1, 8))(x)  # Primary capsules\n",
    "    # Capsule layer\n",
    "    x = CapsuleLayer(num_capsules=32, dim_capsule=8)(x)  # Capsule layer\n",
    "    # Compute the length of capsules\n",
    "    x = layers.Lambda(lambda z: tf.sqrt(tf.reduce_sum(tf.square(z), axis=-1)))(x)  # Shape: [batch_size, num_capsules]\n",
    "    # Classification layer\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)  # Shape: [batch_size, num_classes]\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Build CapsNet model\n",
    "input_shape = (48, 48, 1)  # FER-2013 image size\n",
    "num_classes = 7  # 7 emotions\n",
    "capsnet = build_capsnet(input_shape, num_classes)\n",
    "capsnet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "capsnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<built-in function len> returned a result with an exception set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m     14\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m---> 15\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mcapsnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# plt.imshow(train_images[0].reshape(48, 48), cmap='gray')\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# train_labels[0]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MastersRepos\\Deep-Learning-For-Computer-Vision\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\MastersRepos\\Deep-Learning-For-Computer-Vision\\.venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:77\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_weights:\n\u001b[0;32m     76\u001b[0m     trainable_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_weights\n\u001b[1;32m---> 77\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients, trainable_weights))\n",
      "\u001b[1;31mSystemError\u001b[0m: <built-in function len> returned a result with an exception set"
     ]
    }
   ],
   "source": [
    "# # Data augmentation\n",
    "# datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "# datagen.fit(train_images)\n",
    "\n",
    "# # Train the model\n",
    "# batch_size = 32\n",
    "# epochs = 20\n",
    "# history = capsnet.fit(datagen.flow(train_images, train_labels, batch_size=batch_size),\n",
    "#                       validation_data=(val_images, val_labels),\n",
    "#                       epochs=epochs)\n",
    "\n",
    "# Train the model on original training data without augmentation\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "history = capsnet.fit(train_images, train_labels, \n",
    "                      validation_data=(val_images, val_labels),\n",
    "                      epochs=epochs, \n",
    "                      batch_size=batch_size)\n",
    "\n",
    "# plt.imshow(train_images[0].reshape(48, 48), cmap='gray')\n",
    "# train_labels[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling CapsuleLayer.call().\n\n\u001b[1mDimensions must be equal, but are 32 and 51200 for '{{node functional_1/capsule_layer_5_1/einsum/Einsum}} = Einsum[N=2, T=DT_FLOAT, equation=\"bijc,jcdk->bijdk\"](functional_1/capsule_layer_5_1/Tile, functional_1/capsule_layer_5_1/einsum/Einsum/ReadVariableOp)' with input shapes: [?,51200,32,8], [51200,32,8,8].\u001b[0m\n\nArguments received by CapsuleLayer.call():\n  • inputs=tf.Tensor(shape=(None, 51200, 8), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate on validation set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcapsnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MastersRepos\\Deep-Learning-For-Computer-Vision\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[15], line 21\u001b[0m, in \u001b[0;36mCapsuleLayer.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     19\u001b[0m inputs_expanded \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(inputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     20\u001b[0m inputs_tiled \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtile(inputs_expanded, [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_capsules, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 21\u001b[0m inputs_hat \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbijc,jcdk->bijdk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_tiled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fixed multiplication\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Routing algorithm\u001b[39;00m\n\u001b[0;32m     24\u001b[0m b \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mshape(inputs_hat)[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_num_capsules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_capsules, \u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling CapsuleLayer.call().\n\n\u001b[1mDimensions must be equal, but are 32 and 51200 for '{{node functional_1/capsule_layer_5_1/einsum/Einsum}} = Einsum[N=2, T=DT_FLOAT, equation=\"bijc,jcdk->bijdk\"](functional_1/capsule_layer_5_1/Tile, functional_1/capsule_layer_5_1/einsum/Einsum/ReadVariableOp)' with input shapes: [?,51200,32,8], [51200,32,8,8].\u001b[0m\n\nArguments received by CapsuleLayer.call():\n  • inputs=tf.Tensor(shape=(None, 51200, 8), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "val_loss, val_accuracy = capsnet.evaluate(val_images, val_labels)\n",
    "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(48, 48), color_mode='grayscale')\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "# Predict emotion\n",
    "img_path = 'test_image.jpg'  # Path to your test image\n",
    "img_array = preprocess_image(img_path)\n",
    "prediction = capsnet.predict(img_array)\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "predicted_emotion = emotion_labels[np.argmax(prediction)]\n",
    "print(f\"Predicted Emotion: {predicted_emotion}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
